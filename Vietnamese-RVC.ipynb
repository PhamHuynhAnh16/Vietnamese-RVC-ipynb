{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVAACnvlmE4I"
      },
      "source": [
        "**D·ª± √°n n√†y ƒë∆∞·ª£c n·∫•u b·ªüi [Ph·∫°m Hu·ª≥nh Anh](https://github.com/PhamHuynhAnh16)**\n",
        "\n",
        "# **Vui l√≤ng kh√¥ng s·ª≠ d·ª•ng d·ª± √°n v·ªõi b·∫•t k·ª≥ m·ª•c ƒë√≠ch n√†o vi ph·∫°m ƒë·∫°o ƒë·ª©c, ph√°p lu·∫≠t, ho·∫∑c g√¢y t·ªïn h·∫°i ƒë·∫øn c√° nh√¢n, t·ªï ch·ª©c...**\n",
        "\n",
        "# **Trong tr∆∞·ªùng h·ª£p ng∆∞·ªùi s·ª≠ d·ª•ng kh√¥ng tu√¢n th·ªß c√°c ƒëi·ªÅu kho·∫£n ho·∫∑c vi ph·∫°m, t√¥i s·∫Ω kh√¥ng ch·ªãu tr√°ch nhi·ªám v·ªÅ b·∫•t k·ª≥ khi·∫øu n·∫°i, thi·ªát h·∫°i, hay tr√°ch nhi·ªám ph√°p l√Ω n√†o, d√π l√† trong h·ª£p ƒë·ªìng, do s∆° su·∫•t, hay c√°c l√Ω do kh√°c, ph√°t sinh t·ª´, ngo√†i, ho·∫∑c li√™n quan ƒë·∫øn ph·∫ßn m·ªÅm, vi·ªác s·ª≠ d·ª•ng ph·∫ßn m·ªÅm ho·∫∑c c√°c giao d·ªãch kh√°c li√™n quan ƒë·∫øn ph·∫ßn m·ªÅm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BJeRif5jjL5s"
      },
      "outputs": [],
      "source": [
        "#@title **üåè C√†i ƒë·∫∑t**\n",
        "import os\n",
        "import codecs\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "from threading import Thread\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def download_library_1():\n",
        "  with urllib.request.urlopen(codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/raivebazragf/hfe_1.gne.tm\", \"rot13\")) as response:\n",
        "    with tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n",
        "      tar.extractall(path=\"/usr/local/\")\n",
        "\n",
        "def download_library_2():\n",
        "  with urllib.request.urlopen(codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/raivebazragf/hfe_2.gne.tm\", \"rot13\")) as response:\n",
        "    with tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n",
        "      tar.extractall(path=\"/usr/local/lib/python3.11\")\n",
        "\n",
        "print(\"üë©üèª‚Äçüíª C√†i ƒë·∫∑t...\")\n",
        "\n",
        "download_rvc = Thread(target=lambda: os.system(\"git clone https://github.com/PhamHuynhAnh16/Vietnamese-RVC /content/Vietnamese_RVC\"))\n",
        "download_rvc.start()\n",
        "\n",
        "delete_a = Thread(target=lambda: os.system('rm -rf /usr/local/bin'))\n",
        "delete_b = Thread(target=lambda: os.system('rm -rf /usr/local/lib/python3.11'))\n",
        "\n",
        "delete_a.start()\n",
        "delete_b.start()\n",
        "\n",
        "delete_a.join()\n",
        "delete_b.join()\n",
        "\n",
        "download_a = Thread(target=download_library_1)\n",
        "download_b = Thread(target=download_library_2)\n",
        "\n",
        "download_a.start()\n",
        "download_b.start()\n",
        "\n",
        "download_a.join()\n",
        "download_b.join()\n",
        "\n",
        "#@markdown **üíª C√†i ƒë·∫∑t s·∫Ω m·∫•t kho·∫£ng 2 ph√∫t ƒë·ªÉ ho√†n t·∫•t!**\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIsBEvHaQWMJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **üì± M·ªü giao di·ªán s·ª≠ d·ª•ng**\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "#@markdown **ƒê·ªÉ tr·∫£i nghi·ªám h·∫øt t√≠nh nƒÉng h√£y d√πng giao di·ªán:) C√≤n mu·ªën ƒë∆°n gi·∫£n th√¨ kh√¥ng d√πng giao di·ªán**\n",
        "\n",
        "#@markdown **N·∫øu bi·∫øt c√≥ th·ªÉ s·ª≠ d·ª•ng bi·ªÉu ƒë·ªì ƒë·ªÉ ki·ªÉm tra hu·∫•n luy·ªán qu√° s·ª©c üëç**\n",
        "su_dung_bieu_do = False #@param {type:\"boolean\"}\n",
        "\n",
        "if su_dung_bieu_do:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ./assets/logs --port=6870\n",
        "\n",
        "!python main/app/app.py --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkqks7bO2Cye"
      },
      "source": [
        "# **T√πy ch·ªânh th√™m üß∞**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1cxnr7qP2clf"
      },
      "outputs": [],
      "source": [
        "#@title **K·∫øt n·ªëi ho·∫∑c ng·∫Øt k·∫øt n·ªëi v·ªõi drive ‚òÅ**\n",
        "import os\n",
        "from ipywidgets import Button\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "  print(\"üîó Ng·∫Øt k·∫øt n·ªëi v·ªõi drive...\")\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "    clear_output()\n",
        "    display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))\n",
        "  except Exception as e:\n",
        "    raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh ng·∫Øt k·∫øt n·ªëi drive: {e}')\n",
        "else:\n",
        "  print('üîó K·∫øt n·ªëi v·ªõi drive...')\n",
        "  try:\n",
        "    drive.mount('/content/drive')\n",
        "    clear_output()\n",
        "    display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))\n",
        "  except Exception as e:\n",
        "    raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-pS4-MMA7L9X"
      },
      "outputs": [],
      "source": [
        "#@title **Kh·ªüi ƒë·ªông ho·∫∑c ng·ª´ng sao l∆∞u üõ†**\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "logs_folder, weights_folder, audios_folder = '/content/drive/MyDrive/model/logs', '/content/drive/MyDrive/model/weights', '/content/drive/MyDrive/audios'\n",
        "\n",
        "#@markdown **N·∫øu kh√¥ng t√≠ch v√†o √¥ n√†o th√¨ s·∫Ω ng·ª´ng k·∫øt n·ªëi ·ªü ph·∫ßn ƒë√≥**\n",
        "khoi_dong_sao_luu_mo_hinh = False #@param {\"type\":\"boolean\"}\n",
        "khoi_dong_sao_luu_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **ƒê·ªìng b·ªô l√† s·∫Ω ƒë·ªìng b·ªô c√°c th∆∞ m·ª•c sao l∆∞u l·∫°i, vi·ªác th∆∞ m·ª•c b·ªã x√≥a m·∫•t 1 t·ªáp th√¨ ·ªü th∆∞ m·ª•c sao l∆∞u t·ªáp ƒë√≥ c≈©ng s·∫Ω b·ªã x√≥a**\n",
        "dong_bo_thu_muc = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "class Channel:\n",
        "    def __init__(self, source, destination, sync_deletions=False, every=60, exclude = None):\n",
        "        self.source = source\n",
        "        self.destination = destination\n",
        "        self.event = threading.Event()\n",
        "        self.syncing_thread = threading.Thread(target=self._sync, args=())\n",
        "        self.sync_deletions = sync_deletions\n",
        "        self.every = every\n",
        "\n",
        "        if not exclude: exclude = []\n",
        "        if isinstance(exclude, str): exclude = [exclude]\n",
        "\n",
        "        self.exclude = exclude\n",
        "        self.command = ['rsync', '-aP']\n",
        "\n",
        "    def alive(self):\n",
        "        if self.syncing_thread.is_alive(): return True\n",
        "        else: return False\n",
        "\n",
        "    def _sync(self):\n",
        "        command = self.command\n",
        "\n",
        "        for exclusion in self.exclude:\n",
        "            command.append(f'--exclude={exclusion}')\n",
        "\n",
        "        command.extend([f'{self.source}/', f'{self.destination}/'])\n",
        "\n",
        "        if self.sync_deletions: command.append('--delete')\n",
        "\n",
        "        while not self.event.is_set():\n",
        "            subprocess.run(command)\n",
        "            time.sleep(self.every)\n",
        "\n",
        "    def copy(self):\n",
        "        command = self.command\n",
        "\n",
        "        for exclusion in self.exclude:\n",
        "            command.append(f'--exclude={exclusion}')\n",
        "\n",
        "        command.extend([f'{self.source}/', f'{self.destination}/'])\n",
        "\n",
        "        if self.sync_deletions: command.append('--delete')\n",
        "        subprocess.run(command)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def start(self):\n",
        "        if self.syncing_thread.is_alive():\n",
        "            self.event.set()\n",
        "            self.syncing_thread.join()\n",
        "\n",
        "        if self.event.is_set(): self.event.clear()\n",
        "        if self.syncing_thread._started.is_set(): self.syncing_thread = threading.Thread(target=self._sync, args=())\n",
        "\n",
        "        self.syncing_thread.start()\n",
        "        return self.alive()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.alive():\n",
        "            self.event.set()\n",
        "            self.syncing_thread.join()\n",
        "\n",
        "            while self.alive():\n",
        "                if not self.alive(): break\n",
        "\n",
        "        return not self.alive()\n",
        "\n",
        "if not \"logs_backup\" in locals(): logs_backup = Channel(\"/content/Vietnamese_RVC/assets/logs\", logs_folder, sync_deletions=dong_bo_thu_muc, every=40, exclude=\"mute\")\n",
        "if not \"weights_backup\" in locals(): weights_backup = Channel(\"/content/Vietnamese_RVC/assets/weights\", weights_folder, sync_deletions=dong_bo_thu_muc, every=40)\n",
        "if not \"audio_backup\" in locals(): audio_backup = Channel(\"/content/Vietnamese_RVC/audios\", audios_folder, sync_deletions=dong_bo_thu_muc, every=40)\n",
        "\n",
        "logs_backup.stop(); weights_backup.stop(); audio_backup.stop()\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "  if khoi_dong_sao_luu_mo_hinh:\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder)\n",
        "    if not os.path.exists(weights_folder): os.makedirs(weights_folder)\n",
        "\n",
        "    logs_backup.start(); weights_backup.start()\n",
        "  else: logs_backup.stop(); weights_backup.stop()\n",
        "\n",
        "  if khoi_dong_sao_luu_am_thanh:\n",
        "    if not os.path.exists(audios_folder): os.makedirs(audios_folder)\n",
        "    audio_backup.start()\n",
        "  else: audio_backup.stop()\n",
        "else:\n",
        "  try:\n",
        "    drive.mount('/content/drive')\n",
        "  except Exception as e:\n",
        "      raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')\n",
        "\n",
        "  if khoi_dong_sao_luu_mo_hinh:\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder)\n",
        "    if not os.path.exists(weights_folder): os.makedirs(weights_folder)\n",
        "\n",
        "    logs_backup.start(); weights_backup.start()\n",
        "  else: logs_backup.stop(); weights_backup.stop()\n",
        "\n",
        "  if khoi_dong_sao_luu_am_thanh:\n",
        "    if not os.path.exists(audios_folder): os.makedirs(audios_folder)\n",
        "    audio_backup.start()\n",
        "  else: audio_backup.stop()\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2VfuHALUuW_B"
      },
      "outputs": [],
      "source": [
        "#@title **‚ôªÔ∏è Kh·ªüi ƒë·ªông d·ªçn r√°c**\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import auth, drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  from googleapiclient.discovery import build\n",
        "except:\n",
        "  os.system(\"pip install google-api-python-client\")\n",
        "  from googleapiclient.discovery import build\n",
        "\n",
        "class Clean:\n",
        "    def __init__(self, every=60):\n",
        "        self.service = build('drive', 'v3')\n",
        "        self.every = every\n",
        "        self.trash_cleanup_thread = None\n",
        "\n",
        "    def delete(self):\n",
        "        page_token = None\n",
        "\n",
        "        while 1:\n",
        "            response = self.service.files().list(q=\"trashed=true\", spaces='drive', fields=\"nextPageToken, files(id, name)\", pageToken=page_token).execute()\n",
        "\n",
        "            for file in response.get('files', []):\n",
        "                if file['name'].startswith(\"G_\") and file['name'].endswith(\".pth\") or file['name'].startswith(\"D_\") and file['name'].endswith(\".pth\"):\n",
        "                    try:\n",
        "                        self.service.files().delete(fileId=file['id']).execute()\n",
        "                    except Exception as e:\n",
        "                        raise RuntimeError(e)\n",
        "\n",
        "            page_token = response.get('nextPageToken', None)\n",
        "            if page_token is None: break\n",
        "\n",
        "    def clean(self):\n",
        "        while 1:\n",
        "            self.delete()\n",
        "            time.sleep(self.every)\n",
        "\n",
        "    def start(self):\n",
        "        self.trash_cleanup_thread = threading.Thread(target=self.clean)\n",
        "        self.trash_cleanup_thread.daemon = True\n",
        "        self.trash_cleanup_thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.trash_cleanup_thread: self.trash_cleanup_thread.join()\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "#@markdown **S·ª≠ d·ª•ng khi hu·∫•n luy·ªán s·∫Ω d·ªçn b·ªõt c√°c t·ªáp tin D, G trong th√πng r√°c google drive**\n",
        "khoi_dong_don_rac = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if khoi_dong_don_rac:\n",
        "  if os.path.exists('/content/drive/MyDrive'):\n",
        "    auth.authenticate_user()\n",
        "  else:\n",
        "    try:\n",
        "      drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    Clean(every=40).start()\n",
        "else: Clean().stop()\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xbcVxSFMDOV4"
      },
      "outputs": [],
      "source": [
        "#@title **T·∫£i d·ªØ li·ªáu sao l∆∞u t·ª´ drive üìÇ**\n",
        "import os\n",
        "import shutil\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "logs_folder, weights_folder, audios_folder ='/content/drive/MyDrive/model/logs', '/content/drive/MyDrive/model/weights', '/content/drive/MyDrive/audios'\n",
        "\n",
        "#@markdown **T·∫£i c√°c m√¥ h√¨nh hu·∫•n luy·ªán ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán**\n",
        "tai_mo_hinh = False # @param {\"type\":\"boolean\"}\n",
        "#@markdown **T·∫£i c√°c √¢m thanh ƒë∆∞·ª£c sao l∆∞u ƒë·ªÉ ti·∫øp t·ª•c s·ª≠ d·ª•ng**\n",
        "tai_am_thanh = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive\"):\n",
        "  if tai_mo_hinh:\n",
        "    if len(os.listdir(logs_folder)) < 1 or len(os.listdir(weights_folder)) < 1: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ Li·ªáu\")\n",
        "    else:\n",
        "      if os.path.exists(\"/content/drive/MyDrive/model\"):\n",
        "        shutil.copytree(logs_folder, \"/content/Vietnamese_RVC/assets/logs\", dirs_exist_ok=True)\n",
        "        shutil.copytree(weights_folder, \"/content/Vietnamese_RVC/assets/weights\", dirs_exist_ok=True)\n",
        "\n",
        "        clear_output()\n",
        "      else: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu m√¥ h√¨nh\")\n",
        "  elif tai_am_thanh:\n",
        "    if len(os.listdir(audios_folder)) < 1: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ Li·ªáu\")\n",
        "    else:\n",
        "      if os.path.exists(\"/content/drive/MyDrive/audios\"):\n",
        "        shutil.copytree(audios_folder, \"/content/Vietnamese_RVC/audios\", dirs_exist_ok=True)\n",
        "        clear_output()\n",
        "      else: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu √¢m thanh\")\n",
        "else: print(\"Google drive kh√¥ng ƒë∆∞·ª£c k·∫øt n·ªëi\")\n",
        "\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bMHDuWNgEase"
      },
      "outputs": [],
      "source": [
        "#@title **Dung h·ª£p m√¥ h√¨nhüåÄ**\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def fushion_model(name, pth_1, pth_2, ratio):\n",
        "    if not name: raise ValueError(\"H√£y cung c·∫•p t√™n m√¥ h√¨nh!\")\n",
        "\n",
        "    if not name.endswith(\".pth\"): name = name + \".pth\"\n",
        "\n",
        "    if not pth_1 or not os.path.exists(pth_1) or not pth_1.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh 1 h·ª£p l·ªá\")\n",
        "    if not pth_2 or not os.path.exists(pth_2) or not pth_1.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh 2 h·ª£p l·ªá\")\n",
        "\n",
        "    def extract(ckpt):\n",
        "        a = ckpt[\"model\"]\n",
        "        opt = OrderedDict()\n",
        "        opt[\"weight\"] = {}\n",
        "\n",
        "        for key in a.keys():\n",
        "            if \"enc_q\" in key: continue\n",
        "\n",
        "            opt[\"weight\"][key] = a[key]\n",
        "\n",
        "        return opt\n",
        "\n",
        "    try:\n",
        "        ckpt1 = torch.load(pth_1, map_location=\"cpu\")\n",
        "        ckpt2 = torch.load(pth_2, map_location=\"cpu\")\n",
        "\n",
        "        if ckpt1[\"sr\"] != ckpt2[\"sr\"]: raise ValueError(\"T·ªëc ƒë·ªô l·∫•y m·∫´u c·ªßa 2 m√¥ h√¨nh kh√¥ng gi·ªëng nhau\")\n",
        "\n",
        "        cfg = ckpt1[\"config\"]\n",
        "        cfg_f0 = ckpt1[\"f0\"]\n",
        "        cfg_version = ckpt1[\"version\"]\n",
        "        cfg_sr = ckpt1[\"sr\"]\n",
        "\n",
        "        ckpt1 = extract(ckpt1) if \"model\" in ckpt1 else ckpt1[\"weight\"]\n",
        "        ckpt2 = extract(ckpt2) if \"model\" in ckpt2 else ckpt2[\"weight\"]\n",
        "\n",
        "        if sorted(list(ckpt1.keys())) != sorted(list(ckpt2.keys())): raise ValueError(\"C·∫•u tr√∫c m√¥ h√¨nh kh√¥ng gi·ªëng nhau\")\n",
        "\n",
        "        opt = OrderedDict()\n",
        "        opt[\"weight\"] = {}\n",
        "\n",
        "        for key in ckpt1.keys():\n",
        "            if key == \"emb_g.weight\" and ckpt1[key].shape != ckpt2[key].shape:\n",
        "                min_shape0 = min(ckpt1[key].shape[0], ckpt2[key].shape[0])\n",
        "                opt[\"weight\"][key] = (ratio * (ckpt1[key][:min_shape0].float()) + (1 - ratio) * (ckpt2[key][:min_shape0].float())).half()\n",
        "            else: opt[\"weight\"][key] = (ratio * (ckpt1[key].float()) + (1 - ratio) * (ckpt2[key].float())).half()\n",
        "\n",
        "        opt[\"config\"] = cfg\n",
        "        opt[\"sr\"] = cfg_sr\n",
        "        opt[\"f0\"] = cfg_f0\n",
        "        opt[\"version\"] = cfg_version\n",
        "        opt[\"infos\"] = \"BRUH\"\n",
        "\n",
        "        output_model = os.path.join(\"assets\", \"weights\")\n",
        "        if not os.path.exists(output_model): os.makedirs(output_model, exist_ok=True)\n",
        "\n",
        "        torch.save(opt, os.path.join(output_model, name))\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói: {e}\")\n",
        "\n",
        "def upload_model_files():\n",
        "    uploaded = files.upload()\n",
        "    for name, data in uploaded.items():\n",
        "        with open(name, 'wb') as f:\n",
        "            f.write(data)\n",
        "        print(f'T·∫£i L√™n {name}')\n",
        "\n",
        "    return uploaded\n",
        "\n",
        "#@markdown **T√™n c·ªßa t·ªáp m√¥ h√¨nh khi l∆∞u ra**\n",
        "ten_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "#@markdown **ƒê∆∞·ªùng d·∫´n c·ªßa 2 m√¥ h√¨nh c·∫ßn dung h·ª£p**\n",
        "tep_mo_hinh_1 = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My_Model_1.pth\"}\n",
        "tep_mo_hinh_2 = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My_Model_2.pth\"}\n",
        "#@markdown **Ch·ªânh h∆∞·ªõng v·ªÅ b√™n n√†o s·∫Ω l√†m cho m√¥ h√¨nh gi·ªëng v·ªõi b√™n ƒë√≥**\n",
        "ty_le_mo_hinh = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "if not tep_mo_hinh_1:\n",
        "    print(\"H√£y t·∫£i l√™n m√¥ h√¨nh th·ª© 1:\")\n",
        "    uploaded_files_a = upload_model_files()\n",
        "    duong_dan_mo_hinh_1 = list(uploaded_files_a.keys())[0]\n",
        "elif not tep_mo_hinh_2:\n",
        "    print(\"H√£y t·∫£i l√™n m√¥ h√¨nh th·ª© 2:\")\n",
        "    uploaded_files_b = upload_model_files()\n",
        "    duong_dan_mo_hinh_2 = list(uploaded_files_b.keys())[0]\n",
        "else:\n",
        "    duong_dan_mo_hinh_1 = os.path.join(tep_mo_hinh_1)\n",
        "    duong_dan_mo_hinh_2 = os.path.join(tep_mo_hinh_2)\n",
        "\n",
        "fushion_model(ten_mo_hinh, duong_dan_mo_hinh_1, duong_dan_mo_hinh_2, ty_le_mo_hinh)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RM_z1GlVEq61"
      },
      "outputs": [],
      "source": [
        "#@title **ƒê·ªçc th√¥ng tin m√¥ h√¨nhüì∞**\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "duong_dan_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My-Model.pth\"}\n",
        "\n",
        "def model_info(path):\n",
        "    if not path or not os.path.exists(path): raise ValueError(\"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh!\")\n",
        "\n",
        "    def prettify_date(date_str):\n",
        "        if date_str == \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\": return None\n",
        "\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%f\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        except ValueError:\n",
        "            return \"ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá\"\n",
        "\n",
        "    model_data = torch.load(path, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    print(f\"C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c ·ª©ng d·ª•ng kh√°c nhau c√≥ th·ªÉ ƒëem l·∫°i c√°c th√¥ng tin kh√°c nhau ho·∫∑c kh√¥ng th·ªÉ ƒë·ªçc!\")\n",
        "\n",
        "    epochs = model_data.get(\"epoch\", None)\n",
        "\n",
        "    if epochs is None:\n",
        "        epochs = model_data.get(\"info\", None)\n",
        "        epoch = epochs.replace(\"epoch\", \"\").replace(\"e\", \"\").isdigit()\n",
        "\n",
        "        if epoch and epochs is None: epochs = \"Kh√¥ng t√¨m th·∫•y k·ª∑ nguy√™n\"\n",
        "\n",
        "    steps = model_data.get(\"step\", \"Kh√¥ng t√¨m th·∫•y\")\n",
        "\n",
        "    sr = model_data.get(\"sr\", \"Kh√¥ng t√¨m th·∫•y t·ªëc ƒë·ªô l·∫•y m·∫´u\")\n",
        "    f0 = model_data.get(\"f0\", \"Kh√¥ng t√¨m th·∫•y hu·∫•n luy·ªán cao ƒë·ªô\")\n",
        "\n",
        "    version = model_data.get(\"version\", \"Kh√¥ng t√¨m th·∫•y phi√™n b·∫£n\")\n",
        "    creation_date = model_data.get(\"creation_date\", \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\")\n",
        "    model_hash = model_data.get(\"model_hash\", \"Kh√¥ng t√¨m th·∫•y\")\n",
        "\n",
        "    pitch_guidance = \"ƒê∆∞·ª£c hu·∫•n luy·ªán cao ƒë·ªô\" if f0 else \"Kh√¥ng ƒë∆∞·ª£c hu·∫•n luy·ªán cao ƒë·ªô\"\n",
        "\n",
        "    creation_date_str = prettify_date(creation_date) if creation_date else \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\"\n",
        "\n",
        "    model_name = model_data.get(\"model_name\", \"M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c ghi ch√©p\")\n",
        "    model_author = model_data.get(\"author\", \"M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c ghi ch√©p\")\n",
        "\n",
        "    print(\"Ho√†n th√†nh\")\n",
        "\n",
        "    return (\n",
        "        f\"T√™n m√¥ h√¨nh: {model_name}\\n\"\n",
        "        f\"Ng∆∞·ªùi t·∫°o m√¥ h√¨nh: {model_author}\\n\"\n",
        "        f\"K·ª∑ nguy√™n: {epochs}\\n\"\n",
        "        f\"S·ªë b∆∞·ªõc: {steps}\\n\"\n",
        "        f\"Phi√™n b·∫£n c·ªßa m√¥ h√¨nh: {version}\\n\"\n",
        "        f\"T·ªëc ƒë·ªô l·∫•y m·∫´u: {sr}\\n\"\n",
        "        f\"Hu·∫•n luy·ªán cao ƒë·ªô: {pitch_guidance}\\n\"\n",
        "        f\"Hash (ID): {model_hash}\\n\"\n",
        "        f\"Th·ªùi gian t·∫°o: {creation_date_str}\\n\"\n",
        "    )\n",
        "\n",
        "if not duong_dan_mo_hinh:\n",
        "    uploaded = files.upload()\n",
        "    duong_dan_mo_hinh = list(uploaded.keys())[0]\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "print(model_info(duong_dan_mo_hinh))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP4ifZaG_yd5"
      },
      "source": [
        "# **T√°ch nh·∫°c üéº**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TJx4bDf6b811"
      },
      "outputs": [],
      "source": [
        "#@title **T√°ch Nh·∫°c**\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import yt_dlp\n",
        "import warnings\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def download_url(url):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        ydl_opts = {\"format\": \"bestaudio/best\", \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\", \"preferredquality\": \"192\"}], \"quiet\": True, \"no_warnings\": True, \"noplaylist\": True, \"verbose\": False}\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            audio_output = os.path.join(\"audios\", re.sub(r'\\s+', '-', re.sub(r'[^\\w\\s\\u4e00-\\u9fff\\uac00-\\ud7af\\u0400-\\u04FF\\u1100-\\u11FF]', '', ydl.extract_info(url, download=False).get('title', 'video')).strip()))\n",
        "            if os.path.exists(audio_output): shutil.rmtree(audio_output, ignore_errors=True)\n",
        "\n",
        "            ydl_opts['outtmpl'] = audio_output.replace(\".wav\", \"\")\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            audio_output = audio_output + \".wav\"\n",
        "            if os.path.exists(audio_output): os.remove(audio_output)\n",
        "\n",
        "            ydl.download([url])\n",
        "\n",
        "        return audio_output\n",
        "\n",
        "def separator_music(input, format, segments_size, overlap, separator_model, backing, reverb):\n",
        "    filename, _ = os.path.splitext(os.path.basename(input))\n",
        "    output = os.path.join(\"audios\", filename)\n",
        "\n",
        "    if not os.path.exists(output): os.makedirs(output)\n",
        "    backing_reverb = backing and reverb\n",
        "\n",
        "    !python main/inference/separator_music.py --input_path $input --output_path $output --format $format --shifts 2 --segments_size $segments_size --overlap $overlap --mdx_hop_length 1024 --mdx_batch_size 1 --clean_audio False --clean_strength 0.7 --kara_model 'Version-2' --backing $backing --mdx_denoise True --reverb $reverb --backing_reverb $backing_reverb --model_name $separator_model --sample_rate 44100\n",
        "\n",
        "def is_url(path):\n",
        "    try:\n",
        "        result = urlparse(path)\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ t√°ch b√® v√† t√°ch vang khi t√°ch nh·∫°c**\n",
        "tach_be = False # @param {\"type\":\"boolean\"}\n",
        "tach_vang = False # @param {\"type\":\"boolean\"}\n",
        "#@markdown **ƒê∆∞·ªùng d·∫´n ƒë·∫ßu v√†o (c√≥ th·ªÉ l√† ƒë∆∞·ªùng d·∫´n li√™n k·∫øt ho·∫∑c ƒë∆∞·ªùng d·∫´n t·ªáp) v√† ƒë·ªãnh d·∫°ng ƒë·∫ßu ra t·ªáp**\n",
        "duong_dan = \"\" #@param {\"type\":\"string\", \"placeholder\":\"Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp √¢m thanh ho·∫∑c ƒë∆∞·ªùng d·∫´n li√™n k·∫øt √¢m thanh\"}\n",
        "dinh_dang_am_thanh = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ m√¥ h√¨nh t√°ch nh·∫°c, m·ª©c ch·ªìng ch√©o v√† k√≠ch th∆∞·ªõc ph√¢n ƒëo·∫°n**\n",
        "mo_hinh_tach_nhac = \"Voc_FT\" #@param [\"Main_340\", \"Main_390\", \"Main_406\", \"Main_427\", \"Main_438\", \"Inst_full_292\", \"Inst_HQ_1\", \"Inst_HQ_2\", \"Inst_HQ_3\", \"Inst_HQ_4\", \"Inst_HQ_5\", \"Kim_Vocal_1\", \"Kim_Vocal_2\", \"Kim_Inst\", \"Inst_187_beta\", \"Inst_82_beta\", \"Inst_90_beta\", \"Voc_FT\", \"Crowd_HQ\", \"Inst_1\", \"Inst_2\", \"Inst_3\", \"MDXNET_1_9703\", \"MDXNET_2_9682\", \"MDXNET_3_9662\", \"Inst_Main\", \"MDXNET_Main\", \"MDXNET_9482\", \"HT-Normal\", \"HT-Tuned\", \"HD_MMI\",  \"HT_6S\"]\n",
        "chong_cheo = \"0.25\" # @param [\"0.25\", \"0.5\", \"0.75\", \"0.99\"]\n",
        "kich_thuoc_phan_doan = 256 # @param {\"type\":\"slider\",\"min\":32,\"max\":2048,\"step\":8}\n",
        "\n",
        "%cd /content/Vietnamese_RVC/\n",
        "\n",
        "if not duong_dan:\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_path = os.path.join(\"audios\", os.path.basename(filename).replace(' ', '_').replace('(', '').replace(')', '').replace('{', '').replace('}', ''))\n",
        "    shutil.move(filename, input_path)\n",
        "else: input_path = download_url(duong_dan) if is_url(duong_dan) else duong_dan\n",
        "\n",
        "separator_music(input_path, dinh_dang_am_thanh, kich_thuoc_phan_doan, chong_cheo, mo_hinh_tach_nhac, tach_be, tach_vang)\n",
        "clear_output()\n",
        "\n",
        "filename, _ = os.path.splitext(os.path.basename(input_path))\n",
        "print(f\"T·ªáp ƒë·∫ßu ra c·ªßa b·∫°n n·∫±m trong th∆∞ m·ª•c: /content/Vietnamese_RVC/audios/{filename}\")\n",
        "\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekfkFFNqppfM"
      },
      "source": [
        "# **Chuy·ªÉn ƒë·ªïi √¢m thanh üîä**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DSMX-pHjb9Zr"
      },
      "outputs": [],
      "source": [
        "#@title **üîé T√¨m ki·∫øm m√¥ h√¨nh**\n",
        "import json\n",
        "import codecs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_models_data(search):\n",
        "    all_table_data = []\n",
        "    page = 1\n",
        "\n",
        "    while 1:\n",
        "        try:\n",
        "            response = requests.post(url=codecs.decode(\"uggcf://ibvpr-zbqryf.pbz/srgpu_qngn.cuc\", \"rot13\"), data={\"page\": page, \"search\": search})\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                table_data = response.json().get(\"table\", \"\")\n",
        "                if not table_data.strip(): break\n",
        "                all_table_data.append(table_data)\n",
        "                page += 1\n",
        "            else: raise Exception(f\"ƒê√£ x·∫£y ra l·ªói, M√£ l·ªói: {response.status_code}\")\n",
        "        except json.JSONDecodeError:\n",
        "            raise Exception(\"ƒê√£ x·∫£y ra l·ªói kh√¥ng th·ªÉ ph√¢n t√≠ch t·ª´ ph·∫£n h·ªìi.\")\n",
        "        except requests.RequestException as e:\n",
        "            raise Exception(f\"G·ª≠i y√™u c·∫ßu th·∫•t b·∫°i: {e}\")\n",
        "    return all_table_data\n",
        "\n",
        "def search_models(name):\n",
        "    tables = fetch_models_data(name)\n",
        "\n",
        "    if len(tables) == 0: print(\"Kh√¥ng t√¨m th·∫•y...\")\n",
        "    else:\n",
        "        for table in tables:\n",
        "            for row in BeautifulSoup(table, \"html.parser\").select(\"tr\"):\n",
        "                name_tag, url_tag = row.find(\"a\", {\"class\": \"fs-5\"}), row.find(\"a\", {\"class\": \"btn btn-sm fw-bold btn-light ms-0 p-1 ps-2 pe-2\"})\n",
        "                if name_tag and url_tag:\n",
        "                    name = name_tag.text.replace(\".pth\", \"\").replace(\".index\", \"\").replace(\".zip\", \"\").replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip()\n",
        "                    url = url_tag[\"href\"].replace(\"https://easyaivoice.com/run?url=\", \"\")\n",
        "                    print(f\"{name}: {url}\")\n",
        "\n",
        "#@markdown **T√¨m ki·∫øm li√™n k·∫øt m√¥ h√¨nh**\n",
        "ten_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n c·ªßa m√¥ h√¨nh c·∫ßn t√¨m\"}\n",
        "search_models(ten_mo_hinh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y7KVxsDcb9mU"
      },
      "outputs": [],
      "source": [
        "#@title **üì© T·∫£i xu·ªëng m√¥ h√¨nh**\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def move_files_from_directory(src_dir, dest_weights, dest_logs, model_name):\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file.endswith(\".index\"):\n",
        "                model_log_dir = os.path.join(dest_logs, model_name)\n",
        "                os.makedirs(model_log_dir, exist_ok=True)\n",
        "\n",
        "                filepath = os.path.join(model_log_dir, file.replace(' ', '_').replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip())\n",
        "                if os.path.exists(filepath): os.remove(filepath)\n",
        "                shutil.move(file_path, filepath)\n",
        "            elif file.endswith(\".pth\") and not file.startswith(\"D_\") and not file.startswith(\"G_\"):\n",
        "                pth_path = os.path.join(dest_weights, model_name + \".pth\")\n",
        "                if os.path.exists(pth_path): os.remove(pth_path)\n",
        "                shutil.move(file_path, pth_path)\n",
        "\n",
        "def download_model(url=None, model=None):\n",
        "    if not url: raise ValueError(\"H√£y cung c·∫•p ƒë∆∞·ªùng d·∫´n li√™n k·∫øt m√¥ h√¨nh h·ª£p l·ªá!\")\n",
        "    if not model: raise ValueError(\"H√£y cung c·∫•p t√™n m√¥ h√¨nh ƒë·ªÉ t·∫£i v·ªÅ!\")\n",
        "\n",
        "    model = model.replace(\".pth\", \"\").replace(\".index\", \"\").replace(\".zip\", \"\").replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip()\n",
        "    url = url.replace(\"/blob/\", \"/resolve/\").replace(\"?download=true\", \"\").strip()\n",
        "\n",
        "    download_dir = os.path.join(\"download_model\")\n",
        "    weights_dir = os.path.join(\"assets\", \"weights\")\n",
        "    logs_dir = os.path.join(\"assets\", \"logs\")\n",
        "\n",
        "    if not os.path.exists(download_dir): os.makedirs(download_dir, exist_ok=True)\n",
        "    if not os.path.exists(weights_dir): os.makedirs(weights_dir, exist_ok=True)\n",
        "    if not os.path.exists(logs_dir): os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        from main.tools import gdown, meganz, mediafire, pixeldrain, huggingface\n",
        "\n",
        "        if url.endswith(\".pth\"): huggingface.HF_download_file(url, os.path.join(weights_dir, f\"{model}.pth\"))\n",
        "        elif url.endswith(\".index\"):\n",
        "            model_log_dir = os.path.join(logs_dir, model)\n",
        "            os.makedirs(model_log_dir, exist_ok=True)\n",
        "\n",
        "            huggingface.HF_download_file(url, os.path.join(model_log_dir, f\"{model}.index\"))\n",
        "        elif url.endswith(\".zip\"):\n",
        "            output_path = huggingface.HF_download_file(url, os.path.join(download_dir, model + \".zip\"))\n",
        "            shutil.unpack_archive(output_path, download_dir)\n",
        "\n",
        "            move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "        else:\n",
        "            if \"drive.google.com\" in url or \"drive.usercontent.google.com\" in url:\n",
        "                file_id = None\n",
        "\n",
        "                if \"/file/d/\" in url: file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
        "                elif \"open?id=\" in url: file_id = url.split(\"open?id=\")[1].split(\"/\")[0]\n",
        "                elif \"/download?id=\" in url: file_id = url.split(\"/download?id=\")[1].split(\"&\")[0]\n",
        "\n",
        "                if file_id:\n",
        "                    file = gdown.gdown_download(id=file_id, output=download_dir)\n",
        "                    if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                    move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"mega.nz\" in url:\n",
        "                meganz.mega_download_url(url, download_dir)\n",
        "\n",
        "                file_download = next((f for f in os.listdir(download_dir)), None)\n",
        "                if file_download.endswith(\".zip\"): shutil.unpack_archive(os.path.join(download_dir, file_download), download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"mediafire.com\" in url:\n",
        "                file = mediafire.Mediafire_Download(url, download_dir)\n",
        "                if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"pixeldrain.com\" in url:\n",
        "                file = pixeldrain.pixeldrain(url, download_dir)\n",
        "                if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            else: raise ValueError(\"Li√™n k·∫øt kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£\")\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói: {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(download_dir, ignore_errors=True)\n",
        "\n",
        "def save_drop_model(dropbox):\n",
        "    weight_folder = os.path.join(\"assets\", \"weights\")\n",
        "    logs_folder = os.path.join(\"assets\", \"logs\")\n",
        "    save_model_temp = os.path.join(\"save_model_temp\")\n",
        "\n",
        "    if not os.path.exists(weight_folder): os.makedirs(weight_folder, exist_ok=True)\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder, exist_ok=True)\n",
        "    if not os.path.exists(save_model_temp): os.makedirs(save_model_temp, exist_ok=True)\n",
        "\n",
        "    shutil.move(dropbox, save_model_temp)\n",
        "\n",
        "    try:\n",
        "        file_name = os.path.basename(dropbox)\n",
        "\n",
        "        if file_name.endswith(\".pth\") and file_name.endswith(\".index\"): raise ValueError(\"T·ªáp b·∫°n v·ª´a t·∫£i l√™n kh√¥ng ph·∫£i m√¥ h√¨nh!\")\n",
        "        else:\n",
        "            if file_name.endswith(\".zip\"):\n",
        "                shutil.unpack_archive(os.path.join(save_model_temp, file_name), save_model_temp)\n",
        "                move_files_from_directory(save_model_temp, weight_folder, logs_folder, file_name.replace(\".zip\", \"\"))\n",
        "            elif file_name.endswith(\".pth\"):\n",
        "                output_file = os.path.join(weight_folder, file_name)\n",
        "                if os.path.exists(output_file): os.remove(output_file)\n",
        "\n",
        "                shutil.move(os.path.join(save_model_temp, file_name), output_file)\n",
        "            elif file_name.endswith(\".index\"):\n",
        "                def extract_name_model(filename):\n",
        "                    match = re.search(r\"([A-Za-z]+)(?=_v|\\.|$)\", filename)\n",
        "                    return match.group(1) if match else None\n",
        "\n",
        "                model_logs = os.path.join(logs_folder, extract_name_model(file_name))\n",
        "                if not os.path.exists(model_logs): os.makedirs(model_logs, exist_ok=True)\n",
        "                shutil.move(os.path.join(save_model_temp, file_name), model_logs)\n",
        "            else: raise Exception(\"Kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c m√¥ h√¨nh!\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(save_model_temp, ignore_errors=True)\n",
        "\n",
        "#@markdown **H·ªó tr·ª£ c√°c li√™n k·∫øt ƒë·∫øn t·ª´ huggingface.co / drive.google.com / mega.nz / mediafire.com**\n",
        "\n",
        "ten_cua_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "lien_ket_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"https://huggingface.co//...\"}\n",
        "\n",
        "if not lien_ket_mo_hinh:\n",
        "    uploaded = files.upload()\n",
        "    save_drop_model(list(uploaded.keys())[0])\n",
        "else:\n",
        "    %cd /content/Vietnamese_RVC\n",
        "\n",
        "    download_model(lien_ket_mo_hinh, ten_cua_mo_hinh)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n t·∫•t!!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fma6vLlab93L",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üéµ **Chuy·ªÉn ƒê·ªïi √Çm Thanh** üéµ\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from ipywidgets import Button\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "from main.library.utils import pydub_convert\n",
        "\n",
        "def convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0_method, input_path, output_path, pth_path, index_path, f0_autotune, clean_audio, clean_strength, export_format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing):\n",
        "    !python main/inference/convert.py --pitch $pitch --filter_radius $filter_radius --index_rate $index_rate --volume_envelope $volume_envelope --protect $protect --hop_length $hop_length --f0_method $f0_method --input_path $input_path --output_path $output_path --pth_path $pth_path --index_path $index_path --f0_autotune $f0_autotune --clean_audio $clean_audio --clean_strength $clean_strength --export_format $export_format --embedder_model $embedder_model --resample_sr $resample_sr --split_audio $split_audio --f0_autotune_strength $f0_autotune_strength --checkpointing $checkpointing\n",
        "\n",
        "def convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, input, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, input_audio_name, checkpointing, apply_effect):\n",
        "    if use_original:\n",
        "        if convert_backing: raise ValueError(\"T·∫Øt chuy·ªÉn ƒë·ªïi gi·ªçng b√® ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng chuy·ªÉn ƒë·ªïi gi·ªçng g·ªëc!\")\n",
        "        elif not_merge_backing: raise ValueError(\"T·∫Øt k·∫øt h·ª£p b√® ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng chuy·ªÉn ƒë·ªïi gi·ªçng g·ªëc!\")\n",
        "\n",
        "    if not model or not os.path.exists(model) or os.path.isdir(model) or not model.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p t·ªáp m√¥ h√¨nh h·ª£p l·ªá!\")\n",
        "\n",
        "    f0method = method if method != \"hybrid\" else hybrid_method\n",
        "    embedder_model = embedders if embedders != \"custom\" else custom_embedders\n",
        "\n",
        "    if use_audio:\n",
        "        output_audio = os.path.join(\"audios\", input_audio_name)\n",
        "\n",
        "        def get_audio_file(label):\n",
        "            matching_files = [f for f in os.listdir(output_audio) if label in f]\n",
        "\n",
        "            if not matching_files: return None\n",
        "            return os.path.join(output_audio, matching_files[0])\n",
        "\n",
        "        output_path = os.path.join(output_audio, f\"Convert_Vocals.{format}\")\n",
        "        output_path_effect = os.path.join(output_audio, f\"Convert_Vocal_Effects.{format}\")\n",
        "        output_backing = os.path.join(output_audio, f\"Convert_Backing.{format}\")\n",
        "        output_merge_backup = os.path.join(output_audio, f\"Vocals+Backing.{format}\")\n",
        "        output_merge_instrument = os.path.join(output_audio, f\"Vocals+Instruments.{format}\")\n",
        "\n",
        "        if os.path.exists(output_audio): os.makedirs(output_audio, exist_ok=True)\n",
        "        if os.path.exists(output_path): os.remove(output_path)\n",
        "\n",
        "        if use_original:\n",
        "            original_vocal = get_audio_file('Original_Vocals_No_Reverb.')\n",
        "\n",
        "            if original_vocal is None: original_vocal = get_audio_file('Original_Vocals.')\n",
        "            if original_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng g·ªëc!\")\n",
        "\n",
        "            input_path = original_vocal\n",
        "        else:\n",
        "            main_vocal = get_audio_file('Main_Vocals_No_Reverb.')\n",
        "            backing_vocal = get_audio_file('Backing_Vocals_No_Reverb.')\n",
        "\n",
        "            if main_vocal is None: main_vocal = get_audio_file('Main_Vocals.')\n",
        "            if not not_merge_backing and backing_vocal is None: backing_vocal = get_audio_file('Backing_Vocals.')\n",
        "\n",
        "            if main_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng ch√≠nh!\")\n",
        "            if not not_merge_backing and backing_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng b√®\")\n",
        "\n",
        "            input_path = main_vocal\n",
        "            backing_path = backing_vocal\n",
        "\n",
        "        print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi gi·ªçng...\")\n",
        "\n",
        "        convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input_path, output_path, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "        if apply_effect:\n",
        "            audio_effects(output_path, output_path_effect, format)\n",
        "            output_path = output_path_effect\n",
        "\n",
        "        print(\"Chuy·ªÉn ƒë·ªïi gi·ªçng ho√†n t·∫•t!\")\n",
        "\n",
        "        if convert_backing:\n",
        "            if os.path.exists(output_backing): os.remove(output_backing)\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi gi·ªçng b√®...\")\n",
        "\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, backing_path, output_backing, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "            print(\"Chuy·ªÉn ƒë·ªïi gi·ªçng b√® ho√†n t·∫•t!\")\n",
        "\n",
        "        if not not_merge_backing and not use_original:\n",
        "            backing_source = output_backing if convert_backing else backing_vocal\n",
        "\n",
        "            if os.path.exists(output_merge_backup): os.remove(output_merge_backup)\n",
        "\n",
        "            pydub_convert(AudioSegment.from_file(output_path)).overlay(pydub_convert(AudioSegment.from_file(backing_source))).export(output_merge_backup, format=format)\n",
        "\n",
        "        if merge_instrument:\n",
        "            vocals = output_merge_backup if not not_merge_backing and not use_original else output_path\n",
        "\n",
        "            if os.path.exists(output_merge_instrument): os.remove(output_merge_instrument)\n",
        "\n",
        "            instruments = get_audio_file('Instruments.')\n",
        "\n",
        "            if instruments is None: output_merge_instrument = None\n",
        "            else: pydub_convert(AudioSegment.from_file(instruments)).overlay(pydub_convert(AudioSegment.from_file(vocals))).export(output_merge_instrument, format=format)\n",
        "    else:\n",
        "        if not input or not os.path.exists(input): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "        if not output: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "\n",
        "        if os.path.isdir(input):\n",
        "            print(\"ƒê∆∞·ªùng d·∫´n ƒë·∫ßu v√†o l√† th∆∞ m·ª•c, chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t ƒë∆∞·ª£c k√≠ch ho·∫°t...\")\n",
        "            if not [f for f in os.listdir(input) if f.lower().endswith((\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"))]: raise ValueError(\"Th∆∞ m·ª•c ƒë·∫ßu v√†o tr·ªëng!\")\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t...\")\n",
        "            output_dir = os.path.dirname(output) or output\n",
        "\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input, output_dir, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "            print(\"Chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t ƒë√£ ho√†n t·∫•t!\")\n",
        "        else:\n",
        "            output_dir = os.path.dirname(output) or output\n",
        "\n",
        "            if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "            if os.path.exists(output): os.remove(output)\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi t·ªáp ƒë·∫ßu v√†o...\")\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input, output, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "            if apply_effect:\n",
        "                filename, _ = os.path.splitext(os.path.basename(output))\n",
        "                audio_effects(output, os.path.join(output_dir, f\"{filename}_effect.{format}\"), format)\n",
        "\n",
        "            print(\"Ho√†n t·∫•t chuy·ªÉn ƒë·ªïi!\")\n",
        "\n",
        "def convert_selection(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, inputp, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, checkpointing, apply_effect):\n",
        "    if use_audio:\n",
        "        choice = [f for f in os.listdir(\"audios\") if os.path.isdir(os.path.join(\"audios\", f))]\n",
        "\n",
        "        if len(choice) == 0: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y b·∫£n t√°ch nh·∫°c n√†o!\")\n",
        "        elif len(choice) == 1:\n",
        "            choice_audio = choice[0]\n",
        "            convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, None, None, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, choice_audio, checkpointing, apply_effect)\n",
        "        else:\n",
        "            print(\"ƒê√£ t√¨m th·∫•y nhi·ªÅu h∆°n 1 b·∫£n t√°ch nh·∫°c, vui l√≤ng ch·ªçn b√†i ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "            for c in choice:\n",
        "                print(f\"{choice.index(c)}. {c}\")\n",
        "\n",
        "            while 1:\n",
        "                try:\n",
        "                    choice_select = int(input(\"H√£y nh·∫≠p s·ªë th·ª© t·ª± c·ªßa b·∫£n t√°ch: \"))\n",
        "\n",
        "                    if 0 <= choice_select < len(choice):\n",
        "                        choice_audio = choice[choice_select]\n",
        "                        print(f\"B·∫°n ƒë√£ ch·ªçn b·∫£n t√°ch: {choice_audio}\")\n",
        "                        break\n",
        "                    else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "                except ValueError:\n",
        "                    print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "                except IndexError:\n",
        "                    print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "            convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, None, None, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, choice_audio, checkpointing, apply_effect)\n",
        "    else: convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, inputp, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, None, checkpointing, apply_effect)\n",
        "    return (f\"/content/Vietnamese_RVC/audios/{choice_audio}/Vocals+Instruments.{format}\" if merge_instrument else (f\"/content/Vietnamese_RVC/audios/{choice_audio}/Vocals+Backing.{format}\" if convert_backing else (f\"/content/Vietnamese_RVC/audios/{choice_audio}/Convert_Vocal_Effects.{format}\" if apply_effect else f\"/content/Vietnamese_RVC/audios/{choice_audio}/Convert_Vocals.{format}\"))) if use_audio else (f\"/content/Vietnamese_RVC/audios/output_effect.{format}\" if apply_effect else f\"/content/Vietnamese_RVC/audios/output.{format}\")\n",
        "\n",
        "def audio_effects(input_path, output_path, export_format):\n",
        "    if not input_path or not os.path.exists(input_path) or os.path.isdir(input_path): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "    if not output_path: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "    output_dir = os.path.dirname(output_path) or output_path\n",
        "\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "    if os.path.exists(output_path): os.remove(output_path)\n",
        "\n",
        "    !python main/inference/audio_effects.py --input_path $input_path --output_path $output_path --reverb_room_size 0.15 --reverb_damping 0.7 --reverb_wet_level 0.2 --reverb_dry_level 0.8 --reverb_width 1.0 --reverb_freeze_mode False --export_format $export_format --reverb True\n",
        "\n",
        "def get_index(model):\n",
        "    return next((f for f in [os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\") and \"trained\" not in name] if model.split(\".\")[0] in f), \"\")\n",
        "\n",
        "#@markdown **C√°c t√πy ch·ªçn s·ª≠ d·ª•ng c√°c b·∫£n ƒë∆∞·ª£c t√°ch nh·∫°c ƒë∆∞·ª£c t√°ch tr∆∞·ªõc ƒë√≥**\n",
        "su_dung_giong_goc = False #@param {\"type\":\"boolean\"}\n",
        "chuyen_doi_giong_be = False #@param {\"type\":\"boolean\"}\n",
        "ket_hop_nhac_nen = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ m√¥ h√¨nh nh∆∞: Cao ƒë·ªô v√† Ch·ªâ m·ª•c**\n",
        "ten_mo_hinh = \"\" #@param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "cao_do = 0 #@param {\"type\":\"slider\",\"min\":-20,\"max\":20,\"step\":1}\n",
        "anh_huong_cua_chi_muc = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "#@markdown **T√πy ch·ªçn ƒë·∫ßu v√†o v√† ƒë·ªãnh d·∫°ng ƒë·∫ßu ra c·ªßa t·ªáp**\n",
        "duong_dan_dau_vao = \"\" #@param {\"type\":\"string\",\"placeholder\":\"ƒê∆∞·ªùng d·∫´n t·ªáp √¢m thanh\"}\n",
        "dinh_dang_tep = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ tr√≠ch xu·∫•t d·ªØ li·ªáu, hop length, b·∫£o v·ªá √¢m thanh v√† t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh tr√≠ch xu·∫•t ƒë·ªÉ ƒë·∫°t ch·∫•t l∆∞·ª£ng cao h∆°n**\n",
        "phuong_phap_trich_xuat = \"rmvpe\" #@param [\"pm\", \"dio\", \"mangio-crepe-tiny\", \"mangio-crepe-tiny-onnx\", \"mangio-crepe-small\", \"mangio-crepe-small-onnx\", \"mangio-crepe-medium\", \"mangio-crepe-medium-onnx\", \"mangio-crepe-large\", \"mangio-crepe-large-onnx\", \"mangio-crepe-full\", \"mangio-crepe-full-onnx\", \"crepe-tiny\", \"crepe-tiny-onnx\", \"crepe-small\", \"crepe-small-onnx\", \"crepe-medium\", \"crepe-medium-onnx\", \"crepe-large\", \"crepe-large-onnx\", \"crepe-full\", \"crepe-full-onnx\", \"fcpe\", \"fcpe-onnx\", \"fcpe-legacy\", \"fcpe-legacy-onnx\", \"rmvpe\", \"rmvpe-onnx\", \"rmvpe-legacy\", \"rmvpe-legacy-onnx\", \"harvest\", \"yin\", \"pyin\"]\n",
        "hop_length = 64 # @param {\"type\":\"slider\",\"min\":64,\"max\":512,\"step\":1}\n",
        "bao_ve_phu_am = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.1}\n",
        "tu_dong_dieu_chinh = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **T√πy ch·ªçn v·ªÅ c·∫Øt √¢m thanh tƒ©nh ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô cao h∆°n v√† √°p d·ª•ng hi·ªáu ·ª©ng vang v√†o √¢m thanh**\n",
        "cat_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "ap_dung_hieu_ung = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if ten_mo_hinh:\n",
        "    model_path = os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}.pth\")\n",
        "    index_path = get_index(f\"{ten_mo_hinh}.pth\".split(\"_\")[0])\n",
        "else:\n",
        "    model_name = sorted(list(model for model in os.listdir(os.path.join(\"assets\", \"weights\")) if model.endswith(\".pth\") and not model.startswith(\"G_\") and not model.startswith(\"D_\")))\n",
        "    indexpath = sorted([os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\")])\n",
        "\n",
        "    if len(model_name) < 1: raise ValueError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "    elif len(model_name) == 1:\n",
        "        model_path = os.path.join(\"assets\", \"weights\", model_name[0])\n",
        "        index_path = get_index(os.path.basename(model_name[0])[0])\n",
        "    else:\n",
        "        print(\"T√¨m th·∫•y nhi·ªÅu h∆°n 1 m√¥ h√¨nh, vui l√≤ng nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "        for m in model_name:\n",
        "            print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "                if model_index < len(model_name):\n",
        "                    selected_model = model_name[model_index]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        model_path = os.path.join(\"assets\", \"weights\", selected_model)\n",
        "        index_path = get_index(os.path.basename(selected_model).split(\"_\")[0])\n",
        "\n",
        "if not index_path: print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c!\")\n",
        "\n",
        "if not duong_dan_dau_vao and not (su_dung_giong_goc or chuyen_doi_giong_be or ket_hop_nhac_nen):\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_path = os.path.join(\"audios\", os.path.basename(filename).replace(' ', '_').replace('(', '').replace(')', '').replace('{', '').replace('}', ''))\n",
        "    shutil.move(filename, input_path)\n",
        "else: input_path = duong_dan_dau_vao\n",
        "\n",
        "output_files = convert_selection(False, tu_dong_dieu_chinh, (su_dung_giong_goc or chuyen_doi_giong_be or ket_hop_nhac_nen), su_dung_giong_goc, chuyen_doi_giong_be, False, ket_hop_nhac_nen, cao_do, 0.7, model_path, index_path, anh_huong_cua_chi_muc, input_path, os.path.join(\"audios\", \"output.wav\"), dinh_dang_tep, phuong_phap_trich_xuat, None, hop_length, \"contentvec_base\", None, 0, 3, 1, bao_ve_phu_am, cat_am_thanh, 1, False, ap_dung_hieu_ung)\n",
        "clear_output()\n",
        "\n",
        "print(f\"T·ªáp ƒë·∫ßu ra c·ªßa b·∫°n l√†: {output_files}\")\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEBxigvvb-Ds",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üéµ **Chuy·ªÉn ƒê·ªïi VƒÉn B·∫£n** üéµ\n",
        "import os\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "from main.tools import edge_tts\n",
        "\n",
        "async def TTS(prompt, voice, speed, output, pitch):\n",
        "    if not prompt: raise ValueError(\"Vui l√≤ng cung c·∫•p vƒÉn b·∫£n ƒë·ªÉ ƒë·ªçc!\")\n",
        "    if not voice: raise ValueError(\"Vui l√≤ng ch·ªçn gi·ªçng n√≥i ƒë·ªÉ chuy·ªÉn ƒë·ªïi!\")\n",
        "    if not output: raise ValueError(\"Vui l√≤ng cung c·∫•p ƒë·∫ßu v√†o h·ª£p l·ªá!\")\n",
        "\n",
        "    if os.path.isdir(output): output = os.path.join(output, f\"tts.wav\")\n",
        "\n",
        "    output_dir = os.path.dirname(output) or output\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    await edge_tts.Communicate(text=prompt, voice=voice, rate=f\"+{speed}%\" if speed >= 0 else f\"{speed}%\", pitch=f\"+{pitch}Hz\" if pitch >= 0 else f\"{pitch}Hz\").save(output)\n",
        "    return output\n",
        "\n",
        "def convert_tts(clean, autotune, pitch, clean_strength, model, index, index_rate, input, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, checkpointing):\n",
        "    if not model or not os.path.exists(model) or os.path.isdir(model) or not model.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p t·ªáp m√¥ h√¨nh h·ª£p l·ªá\")\n",
        "    if not input or not os.path.exists(input): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "    if not output: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "\n",
        "    if os.path.isdir(output): output = os.path.join(output, f\"tts-convert.{format}\")\n",
        "\n",
        "    output_dir = os.path.dirname(output)\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(output): os.remove(output)\n",
        "\n",
        "    f0method = method if method != \"hybrid\" else hybrid_method\n",
        "    embedder_model = embedders if embedders != \"custom\" else custom_embedders\n",
        "\n",
        "    !python main/inference/convert.py --pitch $pitch --filter_radius $filter_radius --index_rate $index_rate --volume_envelope $volume_envelope --protect $protect --hop_length $hop_length --f0_method $f0method --input_path $input --output_path $output --pth_path $model --index_path $index --f0_autotune $autotune --clean_audio $clean --clean_strength $clean_strength --export_format $format --embedder_model $embedder_model --resample_sr $resample_sr --split_audio $split_audio --f0_autotune_strength $f0_autotune_strength --checkpointing $checkpointing\n",
        "\n",
        "def get_index(model):\n",
        "    return next((f for f in [os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\") and \"trained\" not in name] if model.split(\".\")[0] in f), \"\")\n",
        "\n",
        "def process_input(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        file_contents = file.read()\n",
        "\n",
        "    return file_contents\n",
        "\n",
        "#@markdown **VƒÉn b·∫£n c·∫ßn ƒë·ªçc v√† c√°c t√πy ch·ªçn v·ªÅ gi·ªçng ƒë·ªçc v√† t·ªëc ƒë·ªô ƒë·ªçc**\n",
        "van_ban = \"\" # @param {\"type\":\"string\", \"placeholder\":\"VƒÉn b·∫£n c·∫ßn ƒë·ªçc\"}\n",
        "giong_noi = \"vi-VN-NamMinhNeural\" # @param [\"af-ZA-AdriNeural\", \"af-ZA-WillemNeural\", \"sq-AL-AnilaNeural\", \"sq-AL-IlirNeural\", \"am-ET-AmehaNeural\", \"am-ET-MekdesNeural\", \"ar-DZ-AminaNeural\", \"ar-DZ-IsmaelNeural\", \"ar-BH-AliNeural\", \"ar-BH-LailaNeural\", \"ar-EG-SalmaNeural\", \"ar-EG-ShakirNeural\", \"ar-IQ-BasselNeural\", \"ar-IQ-RanaNeural\", \"ar-JO-SanaNeural\", \"ar-JO-TaimNeural\", \"ar-KW-FahedNeural\", \"ar-KW-NouraNeural\", \"ar-LB-LaylaNeural\", \"ar-LB-RamiNeural\", \"ar-LY-ImanNeural\", \"ar-LY-OmarNeural\", \"ar-MA-JamalNeural\", \"ar-MA-MounaNeural\", \"ar-OM-AbdullahNeural\", \"ar-OM-AyshaNeural\", \"ar-QA-AmalNeural\", \"ar-QA-MoazNeural\", \"ar-SA-HamedNeural\", \"ar-SA-ZariyahNeural\", \"ar-SY-AmanyNeural\", \"ar-SY-LaithNeural\", \"ar-TN-HediNeural\", \"ar-TN-ReemNeural\", \"ar-AE-FatimaNeural\", \"ar-AE-HamdanNeural\", \"ar-YE-MaryamNeural\", \"ar-YE-SalehNeural\", \"az-AZ-BabekNeural\", \"az-AZ-BanuNeural\", \"bn-BD-NabanitaNeural\", \"bn-BD-PradeepNeural\", \"bn-IN-BashkarNeural\", \"bn-IN-TanishaaNeural\", \"bs-BA-GoranNeural\", \"bs-BA-VesnaNeural\", \"bg-BG-BorislavNeural\", \"bg-BG-KalinaNeural\", \"my-MM-NilarNeural\", \"my-MM-ThihaNeural\", \"ca-ES-EnricNeural\", \"ca-ES-JoanaNeural\", \"zh-HK-HiuGaaiNeural\", \"zh-HK-HiuMaanNeural\", \"zh-HK-WanLungNeural\", \"zh-CN-XiaoxiaoNeural\", \"zh-CN-XiaoyiNeural\", \"zh-CN-YunjianNeural\", \"zh-CN-YunxiNeural\", \"zh-CN-YunxiaNeural\", \"zh-CN-YunyangNeural\", \"zh-CN-liaoning-XiaobeiNeural\", \"zh-TW-HsiaoChenNeural\", \"zh-TW-YunJheNeural\", \"zh-TW-HsiaoYuNeural\", \"zh-CN-shaanxi-XiaoniNeural\", \"hr-HR-GabrijelaNeural\", \"hr-HR-SreckoNeural\", \"cs-CZ-AntoninNeural\", \"cs-CZ-VlastaNeural\", \"da-DK-ChristelNeural\", \"da-DK-JeppeNeural\", \"nl-BE-ArnaudNeural\", \"nl-BE-DenaNeural\", \"nl-NL-ColetteNeural\", \"nl-NL-FennaNeural\", \"nl-NL-MaartenNeural\", \"en-AU-NatashaNeural\", \"en-AU-WilliamNeural\", \"en-CA-ClaraNeural\", \"en-CA-LiamNeural\", \"en-HK-SamNeural\", \"en-HK-YanNeural\", \"en-IN-NeerjaExpressiveNeural\", \"en-IN-NeerjaNeural\", \"en-IN-PrabhatNeural\", \"en-IE-ConnorNeural\", \"en-IE-EmilyNeural\", \"en-KE-AsiliaNeural\", \"en-KE-ChilembaNeural\", \"en-NZ-MitchellNeural\", \"en-NZ-MollyNeural\", \"en-NG-AbeoNeural\", \"en-NG-EzinneNeural\", \"en-PH-JamesNeural\", \"en-PH-RosaNeural\", \"en-SG-LunaNeural\", \"en-SG-WayneNeural\", \"en-ZA-LeahNeural\", \"en-ZA-LukeNeural\", \"en-TZ-ElimuNeural\", \"en-TZ-ImaniNeural\", \"en-GB-LibbyNeural\", \"en-GB-MaisieNeural\", \"en-GB-RyanNeural\", \"en-GB-SoniaNeural\", \"en-GB-ThomasNeural\", \"en-US-AvaMultilingualNeural\", \"en-US-AndrewMultilingualNeural\", \"en-US-EmmaMultilingualNeural\", \"en-US-BrianMultilingualNeural\", \"en-US-AvaNeural\", \"en-US-AndrewNeural\", \"en-US-EmmaNeural\", \"en-US-BrianNeural\", \"en-US-AnaNeural\", \"en-US-AriaNeural\", \"en-US-ChristopherNeural\", \"en-US-EricNeural\", \"en-US-GuyNeural\", \"en-US-JennyNeural\", \"en-US-MichelleNeural\", \"en-US-RogerNeural\", \"en-US-SteffanNeural\", \"et-EE-AnuNeural\", \"et-EE-KertNeural\", \"fil-PH-AngeloNeural\", \"fil-PH-BlessicaNeural\", \"fi-FI-HarriNeural\", \"fi-FI-NooraNeural\", \"fr-BE-CharlineNeural\", \"fr-BE-GerardNeural\", \"fr-CA-ThierryNeural\", \"fr-CA-AntoineNeural\", \"fr-CA-JeanNeural\", \"fr-CA-SylvieNeural\", \"fr-FR-VivienneMultilingualNeural\", \"fr-FR-RemyMultilingualNeural\", \"fr-FR-DeniseNeural\", \"fr-FR-EloiseNeural\", \"fr-FR-HenriNeural\", \"fr-CH-ArianeNeural\", \"fr-CH-FabriceNeural\", \"gl-ES-RoiNeural\", \"gl-ES-SabelaNeural\", \"ka-GE-EkaNeural\", \"ka-GE-GiorgiNeural\", \"de-AT-IngridNeural\", \"de-AT-JonasNeural\", \"de-DE-SeraphinaMultilingualNeural\", \"de-DE-FlorianMultilingualNeural\", \"de-DE-AmalaNeural\", \"de-DE-ConradNeural\", \"de-DE-KatjaNeural\", \"de-DE-KillianNeural\", \"de-CH-JanNeural\", \"de-CH-LeniNeural\", \"el-GR-AthinaNeural\", \"el-GR-NestorasNeural\", \"gu-IN-DhwaniNeural\", \"gu-IN-NiranjanNeural\", \"he-IL-AvriNeural\", \"he-IL-HilaNeural\", \"hi-IN-MadhurNeural\", \"hi-IN-SwaraNeural\", \"hu-HU-NoemiNeural\", \"hu-HU-TamasNeural\", \"is-IS-GudrunNeural\", \"is-IS-GunnarNeural\", \"id-ID-ArdiNeural\", \"id-ID-GadisNeural\", \"ga-IE-ColmNeural\", \"ga-IE-OrlaNeural\", \"it-IT-GiuseppeNeural\", \"it-IT-DiegoNeural\", \"it-IT-ElsaNeural\", \"it-IT-IsabellaNeural\", \"ja-JP-KeitaNeural\", \"ja-JP-NanamiNeural\", \"jv-ID-DimasNeural\", \"jv-ID-SitiNeural\", \"kn-IN-GaganNeural\", \"kn-IN-SapnaNeural\", \"kk-KZ-AigulNeural\", \"kk-KZ-DauletNeural\", \"km-KH-PisethNeural\", \"km-KH-SreymomNeural\", \"ko-KR-HyunsuNeural\", \"ko-KR-InJoonNeural\", \"ko-KR-SunHiNeural\", \"lo-LA-ChanthavongNeural\", \"lo-LA-KeomanyNeural\", \"lv-LV-EveritaNeural\", \"lv-LV-NilsNeural\", \"lt-LT-LeonasNeural\", \"lt-LT-OnaNeural\", \"mk-MK-AleksandarNeural\", \"mk-MK-MarijaNeural\", \"ms-MY-OsmanNeural\", \"ms-MY-YasminNeural\", \"ml-IN-MidhunNeural\", \"ml-IN-SobhanaNeural\", \"mt-MT-GraceNeural\", \"mt-MT-JosephNeural\", \"mr-IN-AarohiNeural\", \"mr-IN-ManoharNeural\", \"mn-MN-BataaNeural\", \"mn-MN-YesuiNeural\", \"ne-NP-HemkalaNeural\", \"ne-NP-SagarNeural\", \"nb-NO-FinnNeural\", \"nb-NO-PernilleNeural\", \"ps-AF-GulNawazNeural\", \"ps-AF-LatifaNeural\", \"fa-IR-DilaraNeural\", \"fa-IR-FaridNeural\", \"pl-PL-MarekNeural\", \"pl-PL-ZofiaNeural\", \"pt-BR-ThalitaNeural\", \"pt-BR-AntonioNeural\", \"pt-BR-FranciscaNeural\", \"pt-PT-DuarteNeural\", \"pt-PT-RaquelNeural\", \"ro-RO-AlinaNeural\", \"ro-RO-EmilNeural\", \"ru-RU-DmitryNeural\", \"ru-RU-SvetlanaNeural\", \"sr-RS-NicholasNeural\", \"sr-RS-SophieNeural\", \"si-LK-SameeraNeural\", \"si-LK-ThiliniNeural\", \"sk-SK-LukasNeural\", \"sk-SK-ViktoriaNeural\", \"sl-SI-PetraNeural\", \"sl-SI-RokNeural\", \"so-SO-MuuseNeural\", \"so-SO-UbaxNeural\", \"es-AR-ElenaNeural\", \"es-AR-TomasNeural\", \"es-BO-MarceloNeural\", \"es-BO-SofiaNeural\", \"es-CL-CatalinaNeural\", \"es-CL-LorenzoNeural\", \"es-ES-XimenaNeural\", \"es-CO-GonzaloNeural\", \"es-CO-SalomeNeural\", \"es-CR-JuanNeural\", \"es-CR-MariaNeural\", \"es-CU-BelkysNeural\", \"es-CU-ManuelNeural\", \"es-DO-EmilioNeural\", \"es-DO-RamonaNeural\", \"es-EC-AndreaNeural\", \"es-EC-LuisNeural\", \"es-SV-LorenaNeural\", \"es-SV-RodrigoNeural\", \"es-GQ-JavierNeural\", \"es-GQ-TeresaNeural\", \"es-GT-AndresNeural\", \"es-GT-MartaNeural\", \"es-HN-CarlosNeural\", \"es-HN-KarlaNeural\", \"es-MX-DaliaNeural\", \"es-MX-JorgeNeural\", \"es-NI-FedericoNeural\", \"es-NI-YolandaNeural\", \"es-PA-MargaritaNeural\", \"es-PA-RobertoNeural\", \"es-PY-MarioNeural\", \"es-PY-TaniaNeural\", \"es-PE-AlexNeural\", \"es-PE-CamilaNeural\", \"es-PR-KarinaNeural\", \"es-PR-VictorNeural\", \"es-ES-AlvaroNeural\", \"es-ES-ElviraNeural\", \"es-US-AlonsoNeural\", \"es-US-PalomaNeural\", \"es-UY-MateoNeural\", \"es-UY-ValentinaNeural\", \"es-VE-PaolaNeural\", \"es-VE-SebastianNeural\", \"su-ID-JajangNeural\", \"su-ID-TutiNeural\", \"sw-KE-RafikiNeural\", \"sw-KE-ZuriNeural\", \"sw-TZ-DaudiNeural\", \"sw-TZ-RehemaNeural\", \"sv-SE-MattiasNeural\", \"sv-SE-SofieNeural\", \"ta-IN-PallaviNeural\", \"ta-IN-ValluvarNeural\", \"ta-MY-KaniNeural\", \"ta-MY-SuryaNeural\", \"ta-SG-AnbuNeural\", \"ta-SG-VenbaNeural\", \"ta-LK-KumarNeural\", \"ta-LK-SaranyaNeural\", \"te-IN-MohanNeural\", \"te-IN-ShrutiNeural\", \"th-TH-NiwatNeural\", \"th-TH-PremwadeeNeural\", \"tr-TR-AhmetNeural\", \"tr-TR-EmelNeural\", \"uk-UA-OstapNeural\", \"uk-UA-PolinaNeural\", \"ur-IN-GulNeural\", \"ur-IN-SalmanNeural\", \"ur-PK-AsadNeural\", \"ur-PK-UzmaNeural\", \"uz-UZ-MadinaNeural\", \"uz-UZ-SardorNeural\", \"vi-VN-HoaiMyNeural\", \"vi-VN-NamMinhNeural\", \"cy-GB-AledNeural\", \"cy-GB-NiaNeural\", \"zu-ZA-ThandoNeural\", \"zu-ZA-ThembaNeural\"]\n",
        "toc_do_doc = 0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":1}\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ m√¥ h√¨nh nh∆∞: Cao ƒë·ªô v√† Ch·ªâ m·ª•c**\n",
        "ten_mo_hinh = \"\" #@param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "cao_do = 0 #@param {\"type\":\"slider\",\"min\":-20,\"max\":20,\"step\":1}\n",
        "anh_huong_cua_chi_muc = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "#@markdown **C√°c t√πy ch·ªçn v·ªÅ tr√≠ch xu·∫•t d·ªØ li·ªáu, hop length, b·∫£o v·ªá √¢m thanh v√† t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh tr√≠ch xu·∫•t ƒë·ªÉ ƒë·∫°t ch·∫•t l∆∞·ª£ng cao h∆°n**\n",
        "phuong_phap_trich_xuat = \"rmvpe\" #@param [\"pm\", \"dio\", \"mangio-crepe-tiny\", \"mangio-crepe-tiny-onnx\", \"mangio-crepe-small\", \"mangio-crepe-small-onnx\", \"mangio-crepe-medium\", \"mangio-crepe-medium-onnx\", \"mangio-crepe-large\", \"mangio-crepe-large-onnx\", \"mangio-crepe-full\", \"mangio-crepe-full-onnx\", \"crepe-tiny\", \"crepe-tiny-onnx\", \"crepe-small\", \"crepe-small-onnx\", \"crepe-medium\", \"crepe-medium-onnx\", \"crepe-large\", \"crepe-large-onnx\", \"crepe-full\", \"crepe-full-onnx\", \"fcpe\", \"fcpe-onnx\", \"fcpe-legacy\", \"fcpe-legacy-onnx\", \"rmvpe\", \"rmvpe-onnx\", \"rmvpe-legacy\", \"rmvpe-legacy-onnx\", \"harvest\", \"yin\", \"pyin\"]\n",
        "hop_length = 64 # @param {\"type\":\"slider\",\"min\":64,\"max\":512,\"step\":1}\n",
        "bao_ve_phu_am = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.1}\n",
        "tu_dong_dieu_chinh = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **T√πy ch·ªçn v·ªÅ c·∫Øt √¢m thanh tƒ©nh ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô cao h∆°n v√† ƒë·ªãnh d·∫°ng t·ªáp ƒë·∫ßu ra**\n",
        "cat_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "dinh_dang_tep = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "\n",
        "if van_ban: input_text = van_ban\n",
        "else:\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_text = process_input(filename)\n",
        "\n",
        "if ten_mo_hinh:\n",
        "    model_path = os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}.pth\")\n",
        "    index_path = get_index(f\"{ten_mo_hinh}.pth\".split(\"_\")[0])\n",
        "else:\n",
        "    model_name = sorted(list(model for model in os.listdir(os.path.join(\"assets\", \"weights\")) if model.endswith(\".pth\") and not model.startswith(\"G_\") and not model.startswith(\"D_\")))\n",
        "    indexpath = sorted([os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\")])\n",
        "\n",
        "    if len(model_name) < 1: raise ValueError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "    elif len(model_name) == 1:\n",
        "        model_path = os.path.join(\"assets\", \"weights\", model_name[0])\n",
        "        index_path = get_index(os.path.basename(model_name[0])[0])\n",
        "    else:\n",
        "        print(\"T√¨m th·∫•y nhi·ªÅu h∆°n 1 m√¥ h√¨nh, vui l√≤ng nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "        for m in model_name:\n",
        "            print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "                if 0 <= model_index < len(model_name):\n",
        "                    selected_model = model_name[model_index]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        model_path = os.path.join(\"assets\", \"weights\", selected_model)\n",
        "        index_path = get_index(os.path.basename(selected_model).split(\"_\")[0])\n",
        "\n",
        "if not index_path: print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c!\")\n",
        "\n",
        "convert_tts(False, tu_dong_dieu_chinh, cao_do, 0.7, model_path, index_path, anh_huong_cua_chi_muc, await TTS(input_text, giong_noi, toc_do_doc, os.path.join(\"audios\", f\"tts.{dinh_dang_tep}\"), 0), os.path.join(\"audios\", f\"tts-convert.{dinh_dang_tep}\"), dinh_dang_tep, phuong_phap_trich_xuat, None, hop_length, \"contentvec_base\", None, 0, 3, 1, bao_ve_phu_am, cat_am_thanh, 1, False)\n",
        "clear_output()\n",
        "\n",
        "print(f\"T·ªáp ƒë·∫ßu ra c·ªßa b·∫°n l√†: /content/Vietnamese_RVC/audios/tts-convert.{dinh_dang_tep}\")\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ers351v_CMGN"
      },
      "source": [
        "# **Hu·∫•n luy·ªán m√¥ h√¨nh ü§ñ**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì© **T·∫£i xu·ªëng m√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc**\n",
        "#@markdown **Ch·∫°y √¥ ƒë·ªÉ c√≥ th·ªÉ l·ª±a ch·ªçn m√¥ h√¨nh v√† t·ªëc ƒë·ªô l·∫•y m·∫´u c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh t·∫£i x·ªëng**\n",
        "import os\n",
        "import shutil\n",
        "import codecs\n",
        "import requests\n",
        "\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "from main.tools import huggingface\n",
        "\n",
        "def fetch_pretrained_data():\n",
        "    response = requests.get(codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/wfba/phfgbz_cergenvarq.wfba\", \"rot13\"))\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "model_list = list(fetch_pretrained_data().keys())\n",
        "\n",
        "for m in model_list:\n",
        "    print(f\"{model_list.index(m)}. {m}\")\n",
        "\n",
        "while 1:\n",
        "    try:\n",
        "        model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "        if 0 <= model_index < len(model_list):\n",
        "            selected_model = model_list[model_index]\n",
        "            clear_output()\n",
        "            print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "            break\n",
        "        else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "    except ValueError:\n",
        "        print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "    except IndexError:\n",
        "        print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "model_sr_list = list(fetch_pretrained_data()[selected_model].keys())\n",
        "\n",
        "if len(model_sr_list) == 1: selected_sr = model_sr_list[0]\n",
        "else:\n",
        "    for sr in model_sr_list:\n",
        "        print(f\"{model_sr_list.index(sr)}. {sr}\")\n",
        "\n",
        "    while 1:\n",
        "        try:\n",
        "            model_sr_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa t·ªëc ƒë·ªô l·∫•y m·∫´u: \"))\n",
        "            if 0 <= model_sr_index < len(model_sr_list):\n",
        "                selected_sr = model_sr_list[model_sr_index]\n",
        "                print(f\"B·∫°n ƒë√£ ch·ªçn t·ªëc ƒë·ªô l·∫•y m·∫´u: {selected_sr}\")\n",
        "                clear_output()\n",
        "                break\n",
        "            else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "        except ValueError:\n",
        "            print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "        except IndexError:\n",
        "            print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "paths = fetch_pretrained_data()[selected_model][selected_sr]\n",
        "\n",
        "pretraineds_custom_path = os.path.join(\"assets\", \"models\", \"pretrained_custom\")\n",
        "\n",
        "if not os.path.exists(pretraineds_custom_path): os.makedirs(pretraineds_custom_path, exist_ok=True)\n",
        "url = codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/cergenvarq_phfgbz/\", \"rot13\") + paths\n",
        "\n",
        "file = huggingface.HF_download_file(url.replace(\"/blob/\", \"/resolve/\").replace(\"?download=true\", \"\").strip(), os.path.join(pretraineds_custom_path, paths))\n",
        "\n",
        "if file.endswith(\".zip\"):\n",
        "    shutil.unpack_archive(file, pretraineds_custom_path)\n",
        "    os.remove(file)\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n t·∫•t!\", button_style=\"success\")"
      ],
      "metadata": {
        "id": "UkwjwmYDAPjn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqzzBxoK6DeD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üìÅ **T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ ƒë∆∞·ªùng d·∫´n**\n",
        "#@markdown **T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ ƒë∆∞·ªùng d·∫´n li√™n k·∫øt**\n",
        "\n",
        "def create_dataset(input_audio, output_dataset, clean_dataset, clean_strength, separator_reverb, version, overlap, segments_size, denoise_mdx, skip, skip_start, skip_end, hop_length, batch_size, sample_rate):\n",
        "    !python main/inference/create_dataset.py --input_audio $input_audio --output_dataset $output_dataset --clean_dataset $clean_dataset --clean_strength $clean_strength --separator_reverb $separator_reverb --kim_vocal_version $version --overlap $overlap --segments_size $segments_size --mdx_hop_length $hop_length --mdx_batch_size $batch_size --denoise_mdx $denoise_mdx --skip $skip --skip_start_audios $skip_start --skip_end_audios $skip_end --sample_rate $sample_rate\n",
        "\n",
        "lam_sach_du_lieu = False #@param {\"type\":\"boolean\"}\n",
        "duong_dan_lien_ket = \"\" #@param {\"type\":\"string\", placeholder:\"ƒê∆∞·ªùng d·∫´n li√™n k·∫øt\"}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "create_dataset(duong_dan_lien_ket, \"dataset\", lam_sach_du_lieu, 0.7, True, 2, 0.25, 256, True, False, 0, 0, 1024, 1, 48000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üî® **X·ª≠ l√Ω tr√≠ch xu·∫•t d·ªØ li·ªáu** ‚õèÔ∏è\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def preprocess(model_name, sample_rate, cpu_core, cut_preprocess, process_effects, path, clean_dataset, clean_strength):\n",
        "    dataset = os.path.join(path)\n",
        "    sr = int(float(sample_rate.rstrip(\"k\")) * 1000)\n",
        "\n",
        "    if not any(f.lower().endswith((\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\")) for f in os.listdir(dataset) if os.path.isfile(os.path.join(dataset, f))): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu hu·∫•n luy·ªán!\")\n",
        "\n",
        "    model_dir = os.path.join(\"assets\", \"logs\", model_name)\n",
        "    if os.path.exists(model_dir): shutil.rmtree(model_dir, ignore_errors=True)\n",
        "\n",
        "    !python main/inference/preprocess.py --model_name $model_name --dataset_path $dataset --sample_rate $sr --cpu_cores $cpu_core --cut_preprocess $cut_preprocess --process_effects $process_effects --clean_dataset $clean_dataset --clean_strength $clean_strength\n",
        "\n",
        "def extract(model_name, version, method, pitch_guidance, hop_length, cpu_cores, gpu, sample_rate, embedders, custom_embedders):\n",
        "    embedder_model = embedders if embedders != \"custom\" else custom_embedders\n",
        "    sr = int(float(sample_rate.rstrip(\"k\")) * 1000)\n",
        "\n",
        "    model_dir = os.path.join(\"assets\", \"logs\", model_name)\n",
        "    if not any(os.path.isfile(os.path.join(model_dir, \"sliced_audios\", f)) for f in os.listdir(os.path.join(model_dir, \"sliced_audios\"))) or not any(os.path.isfile(os.path.join(model_dir, \"sliced_audios_16k\", f)) for f in os.listdir(os.path.join(model_dir, \"sliced_audios_16k\"))): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω!\")\n",
        "\n",
        "    !python main/inference/extract.py --model_name $model_name --rvc_version $version --f0_method $method --pitch_guidance $pitch_guidance --hop_length $hop_length --cpu_cores $cpu_cores --gpu $gpu --sample_rate $sr --embedder_model $embedder_model\n",
        "\n",
        "def create_index(model_name, rvc_version, index_algorithm):\n",
        "    model_dir = os.path.join(\"assets\", \"logs\", model_name)\n",
        "    if not any(os.path.isfile(os.path.join(model_dir, f\"{rvc_version}_extracted\", f)) for f in os.listdir(os.path.join(model_dir, f\"{rvc_version}_extracted\"))): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t\")\n",
        "\n",
        "    !python main/inference/create_index.py --model_name $model_name --rvc_version $rvc_version --index_algorithm $index_algorithm\n",
        "\n",
        "#@markdown **T·∫£i d·ªØ li·ªáu t·ª´ google drive l√™n d·ªØ li·ªáu c·ªßa google colab ƒë·ªÉ ti·∫øn h√†nh x·ª≠ l√Ω v√† tr√≠ch xu·∫•t**\n",
        "tai_du_lieu_tu_drive = False # @param {\"type\":\"boolean\"}\n",
        "#@markdown **T√™n c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh**\n",
        "ten_mo_hinh = \"\" #@param {\"type\":\"string\", placeholder: \"T√™n m√¥ h√¨nh\"}\n",
        "#@markdown **T·ªëc ƒë·ªô l·∫•y m·∫´u c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh**\n",
        "toc_do_lay_mau = \"48k\" #@param [\"32k\", \"40k\", \"44.1k\", \"48k\"]\n",
        "#@markdown **Ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t cao ƒë·ªô v√† hop length cho m√¥ h√¨nh**\n",
        "phuong_phap_trich_xuat = \"rmvpe\" #@param [\"pm\", \"dio\", \"mangio-crepe-tiny\", \"mangio-crepe-tiny-onnx\", \"mangio-crepe-small\", \"mangio-crepe-small-onnx\", \"mangio-crepe-medium\", \"mangio-crepe-medium-onnx\", \"mangio-crepe-large\", \"mangio-crepe-large-onnx\", \"mangio-crepe-full\", \"mangio-crepe-full-onnx\", \"crepe-tiny\", \"crepe-tiny-onnx\", \"crepe-small\", \"crepe-small-onnx\", \"crepe-medium\", \"crepe-medium-onnx\", \"crepe-large\", \"crepe-large-onnx\", \"crepe-full\", \"crepe-full-onnx\", \"fcpe\", \"fcpe-onnx\", \"fcpe-legacy\", \"fcpe-legacy-onnx\", \"rmvpe\", \"rmvpe-onnx\", \"rmvpe-legacy\", \"rmvpe-legacy-onnx\", \"harvest\", \"yin\", \"pyin\"]\n",
        "hop_length = 64 # @param {\"type\":\"slider\",\"min\":64,\"max\":512,\"step\":1}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "if tai_du_lieu_tu_drive:\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"): raise ValueError(\"B·∫°n ch∆∞a k·∫øt n·ªëi v·ªõi google drive\")\n",
        "    if len([f for f in os.listdir(\"/content/drive/MyDrive/dataset\") if not \".ipynb_checkpoints\" in f]) < 1: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu\")\n",
        "\n",
        "    for audios in os.listdir(\"/content/drive/MyDrive/dataset\"):\n",
        "        shutil.copy(f\"/content/drive/MyDrive/dataset/{audios}\", \"/content/Vietnamese_RVC/dataset\")\n",
        "elif not any(f.lower().endswith((\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\")) for f in os.listdir(\"/content/Vietnamese_RVC/dataset\") if os.path.isfile(os.path.join(\"/content/Vietnamese_RVC/dataset\", f))):\n",
        "    uploaded = files.upload()\n",
        "    for f in list(uploaded.keys()):\n",
        "        input_path = os.path.join(\"dataset\", os.path.basename(f).replace(' ', '_').replace('(', '').replace(')', '').replace('{', '').replace('}', ''))\n",
        "        shutil.move(f, input_path)\n",
        "\n",
        "if not ten_mo_hinh: raise ValueError(\"Vui l√≤ng cung c·∫•p t√™n ƒë·ªÉ ti·∫øn h√†nh!\")\n",
        "\n",
        "preprocess(ten_mo_hinh, toc_do_lay_mau, 2, True, True, \"dataset\", False, 0.7)\n",
        "extract(ten_mo_hinh, \"v2\", phuong_phap_trich_xuat, True, hop_length, 2, 0, toc_do_lay_mau, \"contentvec_base\", None)\n",
        "create_index(ten_mo_hinh, \"v2\", \"Auto\")\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ],
      "metadata": {
        "id": "qwXEOvI0XDvJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgNk1aArb_h8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ü§ñ **Hu·∫•n luy·ªán m√¥ h√¨nh**\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "from main.tools import huggingface\n",
        "\n",
        "def training(model_name, rvc_version, save_every_epoch, save_only_latest, save_every_weights, total_epoch, sample_rate, batch_size, gpu, pitch_guidance, not_pretrain, custom_pretrained, pretrain_g, pretrain_d, detector, threshold, clean_up, cache, model_author, vocoder, checkpointing):\n",
        "    sr = int(float(sample_rate.rstrip(\"k\")) * 1000)\n",
        "    if not model_name: raise ValueError(\"Vui l√≤ng cung c·∫•p t√™n ƒë·ªÉ ti·∫øn h√†nh hu·∫•n luy·ªán!\")\n",
        "\n",
        "    model_dir = os.path.join(\"assets\", \"logs\", model_name)\n",
        "    if not any(os.path.isfile(os.path.join(model_dir, f\"{rvc_version}_extracted\", f)) for f in os.listdir(os.path.join(model_dir, f\"{rvc_version}_extracted\"))): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω!\")\n",
        "\n",
        "    if not not_pretrain:\n",
        "        if not custom_pretrained:\n",
        "            pretrained_selector = {True: {32000: (\"f0G32k.pth\", \"f0D32k.pth\"), 40000: (\"f0G40k.pth\", \"f0D40k.pth\"), 44100: (\"f0G44k.pth\", \"f0D44k.pth\"), 48000: (\"f0G48k.pth\", \"f0D48k.pth\")}, False: {32000: (\"G32k.pth\", \"D32k.pth\"), 40000: (\"G40k.pth\", \"D40k.pth\"), 44100: (\"G44k.pth\", \"D44k.pth\"), 48000: (\"G48k.pth\", \"D48k.pth\")}}\n",
        "            pg, pd = pretrained_selector[pitch_guidance][sr]\n",
        "        else:\n",
        "            if not pretrain_g: raise FileNotFoundError(\"Vui l√≤ng cung c·∫•p hu·∫•n luy·ªán tr∆∞·ªõc G\")\n",
        "            if not pretrain_d: raise FileNotFoundError(\"Vui l√≤ng cung c·∫•p hu·∫•n luy·ªán tr∆∞·ªõc D\")\n",
        "\n",
        "            pg, pd = pretrain_g, pretrain_d\n",
        "\n",
        "        pretrained_G, pretrained_D = (os.path.join(\"assets\", \"models\", f\"pretrained_{rvc_version}\", f\"{vocoder if vocoder != 'Default' else ''}{pg}\"), os.path.join(\"assets\", \"models\", f\"pretrained_{rvc_version}\", f\"{vocoder if vocoder != 'Default' else ''}{pd}\")) if not custom_pretrained else (os.path.join(\"assets\", \"models\", f\"pretrained_custom\", pg), os.path.join(\"assets\", \"models\", f\"pretrained_custom\", pd))\n",
        "        download_version = codecs.decode(f\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/cergenvarq_i{'2' if rvc_version == 'v2' else '1'}/\", \"rot13\")\n",
        "\n",
        "        if not custom_pretrained:\n",
        "            try:\n",
        "                if not os.path.exists(pretrained_G): huggingface.HF_download_file(f\"{download_version}{pg}\", os.path.join(\"assets\", \"models\", f\"pretrained_{rvc_version}\", f\"{vocoder if vocoder != 'Default' else ''}{pg}\"))\n",
        "                if not os.path.exists(pretrained_D): huggingface.HF_download_file(f\"{download_version}{pd}\", os.path.join(\"assets\", \"models\", f\"pretrained_{rvc_version}\", f\"{vocoder if vocoder != 'Default' else ''}{pd}\"))\n",
        "            except:\n",
        "                print(\"S·∫Ω kh√¥ng s·ª≠ d·ª•ng hu·∫•n luy·ªán tr∆∞·ªõc v√¨ th·∫•t b·∫°i trong vi·ªác c√†i ƒë·∫∑t ch√∫ng!\")\n",
        "                pretrained_G, pretrained_D = None, None\n",
        "        else:\n",
        "            if not os.path.exists(pretrained_G): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y hu·∫•n luy·ªán tr∆∞·ªõc G\")\n",
        "            if not os.path.exists(pretrained_D): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y hu·∫•n luy·ªán tr∆∞·ªõc D\")\n",
        "    else: print(\"S·∫Ω kh√¥ng s·ª≠ d·ª•ng hu·∫•n luy·ªán tr∆∞·ªõc!\")\n",
        "\n",
        "    print(\"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\")\n",
        "\n",
        "    !python main/inference/train.py --model_name $model_name --rvc_version $rvc_version --save_every_epoch $save_every_epoch --save_only_latest $save_only_latest --save_every_weights $save_every_weights --total_epoch $total_epoch --sample_rate $sr --batch_size $batch_size --gpu $gpu --pitch_guidance $pitch_guidance --overtraining_detector $detector --overtraining_threshold $threshold --cleanup $clean_up --cache_data_in_gpu $cache --g_pretrained_path $pretrained_G --d_pretrained_path $pretrained_D --model_author $model_author --vocoder $vocoder --checkpointing $checkpointing\n",
        "\n",
        "#@markdown **T√™n c·ªßa m√¥ h√¨nh v√† t·ªëc ƒë·ªô l·∫•y m·∫´u ƒë·ªÉ ti·∫øn h√†nh hu·∫•n luy·ªán m√¥ h√¨nh**\n",
        "ten_mo_hinh = \"Anh\" #@param {\"type\":\"string\", placeholder: \"T√™n m√¥ h√¨nh\"}\n",
        "toc_do_lay_mau = \"48k\" #@param [\"32k\", \"40k\", \"44.1k\", \"48k\"]\n",
        "#@markdown **S·ªë l∆∞·ª£ng k·ª∑ nguy√™n c·ªßa m√¥ h√¨nh v√† bao nhi√™u k·ª∑ nguy√™n s·∫Ω l∆∞u m·ªôt l·∫ßn**\n",
        "so_luong_ky_nguyen = 300 #@param {\"type\":\"slider\", min:1, max:10000}\n",
        "tan_suat_luu = 50 #@param {\"type\":\"slider\", min:1, max:10000}\n",
        "#@markdown **T√πy ch·ªçn v·ªÅ k√≠ch th∆∞·ªõc l√¥ v√† c√≥ s·ª≠ d·ª•ng b·ªô nh·ªõ ƒë·ªám hay kh√¥ng**\n",
        "kich_thuoc_lo = 8 #@param {\"type\":\"slider\", min:1, max:20}\n",
        "bo_nho_dem = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **T√πy ch·ªçn v·ªÅ t·ª± ƒë·ªông ki·ªÉm tra hu·∫•n luy·ªán qu√° s·ª©c v√† s·ª≠ d·ª•ng t√πy ch·ªânh m√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc**\n",
        "kiem_tra_huan_luyen = False #@param {\"type\":\"boolean\"}\n",
        "tuy_chinh_huan_luyen_truoc = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if tuy_chinh_huan_luyen_truoc:\n",
        "    model_name = os.listdir(os.path.join(\"assets\", \"models\", \"pretrained_custom\"))\n",
        "\n",
        "    if len(model_name) >= 2:\n",
        "        for m in model_name:\n",
        "            print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index_d = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh D: \"))\n",
        "                if 0 <= model_index_d < len(model_name):\n",
        "                    selected_model_D = model_name[model_index_d]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh D: {selected_model_D}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index_g = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh G: \"))\n",
        "                if 0 <= model_index_g < len(model_name):\n",
        "                    selected_model_G = model_name[model_index_g]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh G: {selected_model_G}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        if selected_model_D == selected_model_G: raise ValueError(\"M√¥ h√¨nh D v√† G gi·ªëng nhau!\")\n",
        "        clear_output()\n",
        "    else: print(\"kh√¥ng t√¨m th·∫•y m√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc n√†o!\")\n",
        "\n",
        "training(ten_mo_hinh, \"v2\", tan_suat_luu, True, True, so_luong_ky_nguyen, toc_do_lay_mau, kich_thuoc_lo, 0, True, False, tuy_chinh_huan_luyen_truoc, selected_model_G, selected_model_D, kiem_tra_huan_luyen, 50, False, bo_nho_dem, None, \"Default\", False)\n",
        "Button(description=\"\\u2714 Ho√†n t·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4yZIXYyCqZL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üì¶ **N√©n v√† l∆∞u m√¥ h√¨nh** üì¶\n",
        "\n",
        "#@markdown **Ch·∫°y √¥ n√†y v√† ch·ªçn t·ªáp m√¥ h√¨nh c·∫ßn n√©n!**\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def get_index(model):\n",
        "    return next((f for f in [os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\") and \"trained\" not in name] if model.split(\".\")[0] in f), \"\")\n",
        "\n",
        "model_name = sorted(list(model for model in os.listdir(os.path.join(\"assets\", \"weights\")) if model.endswith(\".pth\") and not model.startswith(\"G_\") and not model.startswith(\"D_\")))\n",
        "indexpath = sorted([os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\")])\n",
        "\n",
        "if len(model_name) < 1: raise ValueError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "elif len(model_name) == 1:\n",
        "    model_path = os.path.join(\"assets\", \"weights\", model_name[0])\n",
        "    index_path = get_index(os.path.basename(model_name[0])[0])\n",
        "else:\n",
        "    print(\"T√¨m th·∫•y nhi·ªÅu h∆°n 1 m√¥ h√¨nh, vui l√≤ng nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "    for m in model_name:\n",
        "        print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "    while 1:\n",
        "        try:\n",
        "            model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "            if 0 <= model_index < len(model_name):\n",
        "                selected_model = model_name[model_index]\n",
        "                print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "                break\n",
        "            else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "        except ValueError:\n",
        "            print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "        except IndexError:\n",
        "            print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "    model_path = os.path.join(\"assets\", \"weights\", selected_model)\n",
        "    index_path = get_index(os.path.basename(selected_model).split(\"_\")[0])\n",
        "\n",
        "if not index_path: print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c!\")\n",
        "\n",
        "zip_file_path = os.path.join(\"assets\", os.path.basename(model_path).split(\"_\")[0] + \".zip\")\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "    zipf.write(model_path, os.path.basename(model_path))\n",
        "    if index_path: zipf.write(index_path, os.path.basename(index_path))\n",
        "\n",
        "clear_output()\n",
        "print(f\"ƒê∆∞·ªùng d·∫´n m√¥ h√¨nh c·ªßa b·∫°n: {zip_file_path}\")\n",
        "Button(description=\"\\u2714 Ho√†n t·∫•t!\", button_style=\"success\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tkqks7bO2Cye",
        "XP4ifZaG_yd5",
        "ers351v_CMGN"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
