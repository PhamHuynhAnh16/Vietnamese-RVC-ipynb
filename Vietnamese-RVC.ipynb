{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVAACnvlmE4I"
      },
      "source": [
        "**D·ª± √°n n√†y ƒë∆∞·ª£c n·∫•u b·ªüi [Ph·∫°m Hu·ª≥nh Anh](https://github.com/PhamHuynhAnh16)**\n",
        "\n",
        "# **Vui l√≤ng kh√¥ng s·ª≠ d·ª•ng d·ª± √°n v·ªõi b·∫•t k·ª≥ m·ª•c ƒë√≠ch n√†o vi ph·∫°m ƒë·∫°o ƒë·ª©c, ph√°p lu·∫≠t, ho·∫∑c g√¢y t·ªïn h·∫°i ƒë·∫øn c√° nh√¢n, t·ªï ch·ª©c...**\n",
        "\n",
        "# **Trong tr∆∞·ªùng h·ª£p ng∆∞·ªùi s·ª≠ d·ª•ng kh√¥ng tu√¢n th·ªß c√°c ƒëi·ªÅu kho·∫£n ho·∫∑c vi ph·∫°m, t√¥i s·∫Ω kh√¥ng ch·ªãu tr√°ch nhi·ªám v·ªÅ b·∫•t k·ª≥ khi·∫øu n·∫°i, thi·ªát h·∫°i, hay tr√°ch nhi·ªám ph√°p l√Ω n√†o, d√π l√† trong h·ª£p ƒë·ªìng, do s∆° su·∫•t, hay c√°c l√Ω do kh√°c, ph√°t sinh t·ª´, ngo√†i, ho·∫∑c li√™n quan ƒë·∫øn ph·∫ßn m·ªÅm, vi·ªác s·ª≠ d·ª•ng ph·∫ßn m·ªÅm ho·∫∑c c√°c giao d·ªãch kh√°c li√™n quan ƒë·∫øn ph·∫ßn m·ªÅm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BJeRif5jjL5s"
      },
      "outputs": [],
      "source": [
        "#@title **üåè C√†i ƒë·∫∑t**\n",
        "import os\n",
        "import codecs\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "from threading import Thread\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def download_library_1():\n",
        "  with urllib.request.urlopen(codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/raivebazragf/hfe_1.gne.tm\", \"rot13\")) as response:\n",
        "    with tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n",
        "      tar.extractall(path=\"/usr/local/\")\n",
        "\n",
        "def download_library_2():\n",
        "  with urllib.request.urlopen(codecs.decode(\"uggcf://uhttvatsnpr.pb/NauC/Ivrganzrfr-EIP-Cebwrpg/erfbyir/znva/raivebazragf/hfe_2.gne.tm\", \"rot13\")) as response:\n",
        "    with tarfile.open(fileobj=response, mode=\"r:gz\") as tar:\n",
        "      tar.extractall(path=\"/usr/local/lib/python3.11\")\n",
        "\n",
        "print(\"üë©üèª‚Äçüíª C√†i ƒë·∫∑t...\")\n",
        "\n",
        "download_rvc = Thread(target=lambda: os.system(\"git clone https://github.com/PhamHuynhAnh16/Vietnamese-RVC /content/Vietnamese_RVC\"))\n",
        "download_rvc.start()\n",
        "\n",
        "delete_a = Thread(target=lambda: os.system('rm -rf /usr/local/bin'))\n",
        "delete_b = Thread(target=lambda: os.system('rm -rf /usr/local/lib/python3.11'))\n",
        "\n",
        "delete_a.start()\n",
        "delete_b.start()\n",
        "\n",
        "delete_a.join()\n",
        "delete_b.join()\n",
        "\n",
        "download_a = Thread(target=download_library_1)\n",
        "download_b = Thread(target=download_library_2)\n",
        "\n",
        "download_a.start()\n",
        "download_b.start()\n",
        "\n",
        "download_a.join()\n",
        "download_b.join()\n",
        "\n",
        "#@markdown **üíª C√†i ƒë·∫∑t s·∫Ω m·∫•t kho·∫£ng 2 ph√∫t ƒë·ªÉ ho√†n t·∫•t!**\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIsBEvHaQWMJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **üì± M·ªü giao di·ªán s·ª≠ d·ª•ng**\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "#@markdown **ƒê·ªÉ tr·∫£i nghi·ªám h·∫øt t√≠nh nƒÉng h√£y d√πng giao di·ªán:) C√≤n mu·ªën ƒë∆°n gi·∫£n th√¨ kh√¥ng d√πng giao di·ªán**\n",
        "\n",
        "#@markdown **N·∫øu bi·∫øt c√≥ th·ªÉ s·ª≠ d·ª•ng bi·ªÉu ƒë·ªì ƒë·ªÉ ki·ªÉm tra hu·∫•n luy·ªán qu√° s·ª©c üëç**\n",
        "su_dung_bieu_do = False #@param {type:\"boolean\"}\n",
        "\n",
        "if su_dung_bieu_do:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ./assets/logs --port=6870\n",
        "\n",
        "!python main/app/app.py --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkqks7bO2Cye"
      },
      "source": [
        "# **T√πy ch·ªânh th√™m üß∞**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1cxnr7qP2clf"
      },
      "outputs": [],
      "source": [
        "#@title **K·∫øt n·ªëi ho·∫∑c ng·∫Øt k·∫øt n·ªëi v·ªõi drive ‚òÅ**\n",
        "import os\n",
        "from ipywidgets import Button\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "  print(\"üîó Ng·∫Øt k·∫øt n·ªëi v·ªõi drive...\")\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "    clear_output()\n",
        "    display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))\n",
        "  except Exception as e:\n",
        "    raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh ng·∫Øt k·∫øt n·ªëi drive: {e}')\n",
        "else:\n",
        "  print('üîó K·∫øt n·ªëi v·ªõi drive...')\n",
        "  try:\n",
        "    drive.mount('/content/drive')\n",
        "    clear_output()\n",
        "    display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))\n",
        "  except Exception as e:\n",
        "    raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-pS4-MMA7L9X"
      },
      "outputs": [],
      "source": [
        "#@title **Kh·ªüi ƒë·ªông ho·∫∑c ng·ª´ng sao l∆∞u üõ†**\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "logs_folder, weights_folder, audios_folder = '/content/drive/MyDrive/model/logs', '/content/drive/MyDrive/model/weights', '/content/drive/MyDrive/audios'\n",
        "\n",
        "#@markdown **N·∫øu kh√¥ng t√≠ch v√†o √¥ n√†o th√¨ s·∫Ω ng·ª´ng k·∫øt n·ªëi ·ªü ph·∫ßn ƒë√≥**\n",
        "khoi_dong_sao_luu_mo_hinh = False #@param {\"type\":\"boolean\"}\n",
        "khoi_dong_sao_luu_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **ƒê·ªìng b·ªô l√† s·∫Ω ƒë·ªìng b·ªô c√°c th∆∞ m·ª•c sao l∆∞u l·∫°i, vi·ªác th∆∞ m·ª•c b·ªã x√≥a m·∫•t 1 t·ªáp th√¨ ·ªü th∆∞ m·ª•c sao l∆∞u t·ªáp ƒë√≥ c≈©ng s·∫Ω b·ªã x√≥a**\n",
        "dong_bo_thu_muc = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "class Channel:\n",
        "    def __init__(self, source, destination, sync_deletions=False, every=60, exclude = None):\n",
        "        self.source = source\n",
        "        self.destination = destination\n",
        "        self.event = threading.Event()\n",
        "        self.syncing_thread = threading.Thread(target=self._sync, args=())\n",
        "        self.sync_deletions = sync_deletions\n",
        "        self.every = every\n",
        "\n",
        "        if not exclude: exclude = []\n",
        "        if isinstance(exclude, str): exclude = [exclude]\n",
        "\n",
        "        self.exclude = exclude\n",
        "        self.command = ['rsync', '-aP']\n",
        "\n",
        "    def alive(self):\n",
        "        if self.syncing_thread.is_alive(): return True\n",
        "        else: return False\n",
        "\n",
        "    def _sync(self):\n",
        "        command = self.command\n",
        "\n",
        "        for exclusion in self.exclude:\n",
        "            command.append(f'--exclude={exclusion}')\n",
        "\n",
        "        command.extend([f'{self.source}/', f'{self.destination}/'])\n",
        "\n",
        "        if self.sync_deletions: command.append('--delete')\n",
        "\n",
        "        while not self.event.is_set():\n",
        "            subprocess.run(command)\n",
        "            time.sleep(self.every)\n",
        "\n",
        "    def copy(self):\n",
        "        command = self.command\n",
        "\n",
        "        for exclusion in self.exclude:\n",
        "            command.append(f'--exclude={exclusion}')\n",
        "\n",
        "        command.extend([f'{self.source}/', f'{self.destination}/'])\n",
        "\n",
        "        if self.sync_deletions: command.append('--delete')\n",
        "        subprocess.run(command)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def start(self):\n",
        "        if self.syncing_thread.is_alive():\n",
        "            self.event.set()\n",
        "            self.syncing_thread.join()\n",
        "\n",
        "        if self.event.is_set(): self.event.clear()\n",
        "        if self.syncing_thread._started.is_set(): self.syncing_thread = threading.Thread(target=self._sync, args=())\n",
        "\n",
        "        self.syncing_thread.start()\n",
        "        return self.alive()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.alive():\n",
        "            self.event.set()\n",
        "            self.syncing_thread.join()\n",
        "\n",
        "            while self.alive():\n",
        "                if not self.alive(): break\n",
        "\n",
        "        return not self.alive()\n",
        "\n",
        "if not \"logs_backup\" in locals(): logs_backup = Channel(\"/content/Vietnamese_RVC/assets/logs\", logs_folder, sync_deletions=dong_bo_thu_muc, every=40, exclude=\"mute\")\n",
        "if not \"weights_backup\" in locals(): weights_backup = Channel(\"/content/Vietnamese_RVC/assets/weights\", weights_folder, sync_deletions=dong_bo_thu_muc, every=40)\n",
        "if not \"audio_backup\" in locals(): audio_backup = Channel(\"/content/Vietnamese_RVC/audios\", audios_folder, sync_deletions=dong_bo_thu_muc, every=40)\n",
        "\n",
        "logs_backup.stop(); weights_backup.stop(); audio_backup.stop()\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "  if khoi_dong_sao_luu_mo_hinh:\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder)\n",
        "    if not os.path.exists(weights_folder): os.makedirs(weights_folder)\n",
        "\n",
        "    logs_backup.start(); weights_backup.start()\n",
        "  else: logs_backup.stop(); weights_backup.stop()\n",
        "\n",
        "  if khoi_dong_sao_luu_am_thanh:\n",
        "    if not os.path.exists(audios_folder): os.makedirs(audios_folder)\n",
        "    audio_backup.start()\n",
        "  else: audio_backup.stop()\n",
        "else:\n",
        "  try:\n",
        "    drive.mount('/content/drive')\n",
        "  except Exception as e:\n",
        "      raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')\n",
        "\n",
        "  if khoi_dong_sao_luu_mo_hinh:\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder)\n",
        "    if not os.path.exists(weights_folder): os.makedirs(weights_folder)\n",
        "\n",
        "    logs_backup.start(); weights_backup.start()\n",
        "  else: logs_backup.stop(); weights_backup.stop()\n",
        "\n",
        "  if khoi_dong_sao_luu_am_thanh:\n",
        "    if not os.path.exists(audios_folder): os.makedirs(audios_folder)\n",
        "    audio_backup.start()\n",
        "  else: audio_backup.stop()\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2VfuHALUuW_B"
      },
      "outputs": [],
      "source": [
        "#@title **‚ôªÔ∏è Kh·ªüi ƒë·ªông d·ªçn r√°c**\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import auth, drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  from googleapiclient.discovery import build\n",
        "except:\n",
        "  os.system(\"pip install google-api-python-client\")\n",
        "  from googleapiclient.discovery import build\n",
        "\n",
        "class Clean:\n",
        "    def __init__(self, every=60):\n",
        "        self.service = build('drive', 'v3')\n",
        "        self.every = every\n",
        "        self.trash_cleanup_thread = None\n",
        "\n",
        "    def delete(self):\n",
        "        page_token = None\n",
        "\n",
        "        while 1:\n",
        "            response = self.service.files().list(q=\"trashed=true\", spaces='drive', fields=\"nextPageToken, files(id, name)\", pageToken=page_token).execute()\n",
        "\n",
        "            for file in response.get('files', []):\n",
        "                if file['name'].startswith(\"G_\") and file['name'].endswith(\".pth\") or file['name'].startswith(\"D_\") and file['name'].endswith(\".pth\"):\n",
        "                    try:\n",
        "                        self.service.files().delete(fileId=file['id']).execute()\n",
        "                    except Exception as e:\n",
        "                        raise RuntimeError(e)\n",
        "\n",
        "            page_token = response.get('nextPageToken', None)\n",
        "            if page_token is None: break\n",
        "\n",
        "    def clean(self):\n",
        "        while 1:\n",
        "            self.delete()\n",
        "            time.sleep(self.every)\n",
        "\n",
        "    def start(self):\n",
        "        self.trash_cleanup_thread = threading.Thread(target=self.clean)\n",
        "        self.trash_cleanup_thread.daemon = True\n",
        "        self.trash_cleanup_thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.trash_cleanup_thread: self.trash_cleanup_thread.join()\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "#@markdown **S·ª≠ d·ª•ng khi hu·∫•n luy·ªán s·∫Ω d·ªçn b·ªõt c√°c t·ªáp tin D, G trong th√πng r√°c google drive**\n",
        "khoi_dong_don_rac = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if khoi_dong_don_rac:\n",
        "  if os.path.exists('/content/drive/MyDrive'):\n",
        "    auth.authenticate_user()\n",
        "  else:\n",
        "    try:\n",
        "      drive.mount('/content/drive')\n",
        "    except Exception as e:\n",
        "        raise ValueError(f'ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh k·∫øt n·ªëi drive: {e}')\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    Clean(every=40).start()\n",
        "else: Clean().stop()\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xbcVxSFMDOV4"
      },
      "outputs": [],
      "source": [
        "#@title **T·∫£i d·ªØ li·ªáu sao l∆∞u t·ª´ drive üìÇ**\n",
        "import os\n",
        "import shutil\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "logs_folder, weights_folder, audios_folder ='/content/drive/MyDrive/model/logs', '/content/drive/MyDrive/model/weights', '/content/drive/MyDrive/audios'\n",
        "\n",
        "#@markdown **T·∫£i c√°c m√¥ h√¨nh hu·∫•n luy·ªán ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán**\n",
        "tai_mo_hinh = False # @param {\"type\":\"boolean\"}\n",
        "#@markdown **T·∫£i c√°c √¢m thanh ƒë∆∞·ª£c sao l∆∞u ƒë·ªÉ ti·∫øp t·ª•c s·ª≠ d·ª•ng**\n",
        "tai_am_thanh = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive\"):\n",
        "  if tai_mo_hinh:\n",
        "    if len(os.listdir(logs_folder)) < 1 or len(os.listdir(weights_folder)) < 1: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ Li·ªáu\")\n",
        "    else:\n",
        "      if os.path.exists(\"/content/drive/MyDrive/model\"):\n",
        "        shutil.copytree(logs_folder, \"/content/Vietnamese_RVC/assets/logs\", dirs_exist_ok=True)\n",
        "        shutil.copytree(weights_folder, \"/content/Vietnamese_RVC/assets/weights\", dirs_exist_ok=True)\n",
        "\n",
        "        clear_output()\n",
        "      else: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu m√¥ h√¨nh\")\n",
        "  elif tai_am_thanh:\n",
        "    if len(os.listdir(audios_folder)) < 1: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ Li·ªáu\")\n",
        "    else:\n",
        "      if os.path.exists(\"/content/drive/MyDrive/audios\"):\n",
        "        shutil.copytree(audios_folder, \"/content/Vietnamese_RVC/audios\", dirs_exist_ok=True)\n",
        "        clear_output()\n",
        "      else: print(\"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu √¢m thanh\")\n",
        "else: print(\"Google drive kh√¥ng ƒë∆∞·ª£c k·∫øt n·ªëi\")\n",
        "\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bMHDuWNgEase"
      },
      "outputs": [],
      "source": [
        "#@title **Dung h·ª£p m√¥ h√¨nhüåÄ**\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def fushion_model(name, pth_1, pth_2, ratio):\n",
        "    if not name: raise ValueError(\"H√£y cung c·∫•p t√™n m√¥ h√¨nh!\")\n",
        "\n",
        "    if not name.endswith(\".pth\"): name = name + \".pth\"\n",
        "\n",
        "    if not pth_1 or not os.path.exists(pth_1) or not pth_1.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh 1 h·ª£p l·ªá\")\n",
        "    if not pth_2 or not os.path.exists(pth_2) or not pth_1.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh 2 h·ª£p l·ªá\")\n",
        "\n",
        "    def extract(ckpt):\n",
        "        a = ckpt[\"model\"]\n",
        "        opt = OrderedDict()\n",
        "        opt[\"weight\"] = {}\n",
        "\n",
        "        for key in a.keys():\n",
        "            if \"enc_q\" in key: continue\n",
        "\n",
        "            opt[\"weight\"][key] = a[key]\n",
        "\n",
        "        return opt\n",
        "\n",
        "    try:\n",
        "        ckpt1 = torch.load(pth_1, map_location=\"cpu\")\n",
        "        ckpt2 = torch.load(pth_2, map_location=\"cpu\")\n",
        "\n",
        "        if ckpt1[\"sr\"] != ckpt2[\"sr\"]: raise ValueError(\"T·ªëc ƒë·ªô l·∫•y m·∫´u c·ªßa 2 m√¥ h√¨nh kh√¥ng gi·ªëng nhau\")\n",
        "\n",
        "        cfg = ckpt1[\"config\"]\n",
        "        cfg_f0 = ckpt1[\"f0\"]\n",
        "        cfg_version = ckpt1[\"version\"]\n",
        "        cfg_sr = ckpt1[\"sr\"]\n",
        "\n",
        "        ckpt1 = extract(ckpt1) if \"model\" in ckpt1 else ckpt1[\"weight\"]\n",
        "        ckpt2 = extract(ckpt2) if \"model\" in ckpt2 else ckpt2[\"weight\"]\n",
        "\n",
        "        if sorted(list(ckpt1.keys())) != sorted(list(ckpt2.keys())): raise ValueError(\"C·∫•u tr√∫c m√¥ h√¨nh kh√¥ng gi·ªëng nhau\")\n",
        "\n",
        "        opt = OrderedDict()\n",
        "        opt[\"weight\"] = {}\n",
        "\n",
        "        for key in ckpt1.keys():\n",
        "            if key == \"emb_g.weight\" and ckpt1[key].shape != ckpt2[key].shape:\n",
        "                min_shape0 = min(ckpt1[key].shape[0], ckpt2[key].shape[0])\n",
        "                opt[\"weight\"][key] = (ratio * (ckpt1[key][:min_shape0].float()) + (1 - ratio) * (ckpt2[key][:min_shape0].float())).half()\n",
        "            else: opt[\"weight\"][key] = (ratio * (ckpt1[key].float()) + (1 - ratio) * (ckpt2[key].float())).half()\n",
        "\n",
        "        opt[\"config\"] = cfg\n",
        "        opt[\"sr\"] = cfg_sr\n",
        "        opt[\"f0\"] = cfg_f0\n",
        "        opt[\"version\"] = cfg_version\n",
        "        opt[\"infos\"] = \"BRUH\"\n",
        "\n",
        "        output_model = os.path.join(\"assets\", \"weights\")\n",
        "        if not os.path.exists(output_model): os.makedirs(output_model, exist_ok=True)\n",
        "\n",
        "        torch.save(opt, os.path.join(output_model, name))\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói: {e}\")\n",
        "\n",
        "def upload_model_files():\n",
        "    uploaded = files.upload()\n",
        "    for name, data in uploaded.items():\n",
        "        with open(name, 'wb') as f:\n",
        "            f.write(data)\n",
        "        print(f'T·∫£i L√™n {name}')\n",
        "\n",
        "    return uploaded\n",
        "\n",
        "#@markdown **T√™n c·ªßa t·ªáp m√¥ h√¨nh khi l∆∞u ra**\n",
        "ten_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "#@markdown **ƒê∆∞·ªùng d·∫´n c·ªßa 2 m√¥ h√¨nh c·∫ßn dung h·ª£p**\n",
        "tep_mo_hinh_1 = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My_Model_1.pth\"}\n",
        "tep_mo_hinh_2 = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My_Model_2.pth\"}\n",
        "#@markdown **Ch·ªânh h∆∞·ªõng v·ªÅ b√™n n√†o s·∫Ω l√†m cho m√¥ h√¨nh gi·ªëng v·ªõi b√™n ƒë√≥**\n",
        "ty_le_mo_hinh = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "if not tep_mo_hinh_1:\n",
        "    print(\"H√£y t·∫£i l√™n m√¥ h√¨nh th·ª© 1:\")\n",
        "    uploaded_files_a = upload_model_files()\n",
        "    duong_dan_mo_hinh_1 = list(uploaded_files_a.keys())[0]\n",
        "elif not tep_mo_hinh_2:\n",
        "    print(\"H√£y t·∫£i l√™n m√¥ h√¨nh th·ª© 2:\")\n",
        "    uploaded_files_b = upload_model_files()\n",
        "    duong_dan_mo_hinh_2 = list(uploaded_files_b.keys())[0]\n",
        "else:\n",
        "    duong_dan_mo_hinh_1 = os.path.join(tep_mo_hinh_1)\n",
        "    duong_dan_mo_hinh_2 = os.path.join(tep_mo_hinh_2)\n",
        "\n",
        "fushion_model(ten_mo_hinh, duong_dan_mo_hinh_1, duong_dan_mo_hinh_2, ty_le_mo_hinh)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RM_z1GlVEq61"
      },
      "outputs": [],
      "source": [
        "#@title **ƒê·ªçc th√¥ng tin m√¥ h√¨nhüì∞**\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "duong_dan_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"assets\\\\\\\\weights\\\\\\\\My-Model.pth\"}\n",
        "\n",
        "def model_info(path):\n",
        "    if not path or not os.path.exists(path): raise ValueError(\"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh!\")\n",
        "\n",
        "    def prettify_date(date_str):\n",
        "        if date_str == \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\": return None\n",
        "\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%f\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        except ValueError:\n",
        "            return \"ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá\"\n",
        "\n",
        "    model_data = torch.load(path, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    print(f\"C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c ·ª©ng d·ª•ng kh√°c nhau c√≥ th·ªÉ ƒëem l·∫°i c√°c th√¥ng tin kh√°c nhau ho·∫∑c kh√¥ng th·ªÉ ƒë·ªçc!\")\n",
        "\n",
        "    epochs = model_data.get(\"epoch\", None)\n",
        "\n",
        "    if epochs is None:\n",
        "        epochs = model_data.get(\"info\", None)\n",
        "        epoch = epochs.replace(\"epoch\", \"\").replace(\"e\", \"\").isdigit()\n",
        "\n",
        "        if epoch and epochs is None: epochs = \"Kh√¥ng t√¨m th·∫•y k·ª∑ nguy√™n\"\n",
        "\n",
        "    steps = model_data.get(\"step\", \"Kh√¥ng t√¨m th·∫•y\")\n",
        "\n",
        "    sr = model_data.get(\"sr\", \"Kh√¥ng t√¨m th·∫•y t·ªëc ƒë·ªô l·∫•y m·∫´u\")\n",
        "    f0 = model_data.get(\"f0\", \"Kh√¥ng t√¨m th·∫•y hu·∫•n luy·ªán cao ƒë·ªô\")\n",
        "\n",
        "    version = model_data.get(\"version\", \"Kh√¥ng t√¨m th·∫•y phi√™n b·∫£n\")\n",
        "    creation_date = model_data.get(\"creation_date\", \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\")\n",
        "    model_hash = model_data.get(\"model_hash\", \"Kh√¥ng t√¨m th·∫•y\")\n",
        "\n",
        "    pitch_guidance = \"ƒê∆∞·ª£c hu·∫•n luy·ªán cao ƒë·ªô\" if f0 else \"Kh√¥ng ƒë∆∞·ª£c hu·∫•n luy·ªán cao ƒë·ªô\"\n",
        "\n",
        "    creation_date_str = prettify_date(creation_date) if creation_date else \"Kh√¥ng t√¨m th·∫•y th·ªùi gian t·∫°o\"\n",
        "\n",
        "    model_name = model_data.get(\"model_name\", \"M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c ghi ch√©p\")\n",
        "    model_author = model_data.get(\"author\", \"M√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c ghi ch√©p\")\n",
        "\n",
        "    print(\"Ho√†n th√†nh\")\n",
        "\n",
        "    return (\n",
        "        f\"T√™n m√¥ h√¨nh: {model_name}\\n\"\n",
        "        f\"Ng∆∞·ªùi t·∫°o m√¥ h√¨nh: {model_author}\\n\"\n",
        "        f\"K·ª∑ nguy√™n: {epochs}\\n\"\n",
        "        f\"S·ªë b∆∞·ªõc: {steps}\\n\"\n",
        "        f\"Phi√™n b·∫£n c·ªßa m√¥ h√¨nh: {version}\\n\"\n",
        "        f\"T·ªëc ƒë·ªô l·∫•y m·∫´u: {sr}\\n\"\n",
        "        f\"Hu·∫•n luy·ªán cao ƒë·ªô: {pitch_guidance}\\n\"\n",
        "        f\"Hash (ID): {model_hash}\\n\"\n",
        "        f\"Th·ªùi gian t·∫°o: {creation_date_str}\\n\"\n",
        "    )\n",
        "\n",
        "if not duong_dan_mo_hinh:\n",
        "    uploaded = files.upload()\n",
        "    duong_dan_mo_hinh = list(uploaded.keys())[0]\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "print(model_info(duong_dan_mo_hinh))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP4ifZaG_yd5"
      },
      "source": [
        "# **T√°ch nh·∫°c üéº**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TJx4bDf6b811"
      },
      "outputs": [],
      "source": [
        "#@title **T√°ch Nh·∫°c**\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import yt_dlp\n",
        "import warnings\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def download_url(url):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        ydl_opts = {\"format\": \"bestaudio/best\", \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\", \"preferredquality\": \"192\"}], \"quiet\": True, \"no_warnings\": True, \"noplaylist\": True, \"verbose\": False}\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            audio_output = os.path.join(\"audios\", re.sub(r'\\s+', '-', re.sub(r'[^\\w\\s\\u4e00-\\u9fff\\uac00-\\ud7af\\u0400-\\u04FF\\u1100-\\u11FF]', '', ydl.extract_info(url, download=False).get('title', 'video')).strip()))\n",
        "            if os.path.exists(audio_output): shutil.rmtree(audio_output, ignore_errors=True)\n",
        "\n",
        "            ydl_opts['outtmpl'] = audio_output.replace(\".wav\", \"\")\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            audio_output = audio_output + \".wav\"\n",
        "            if os.path.exists(audio_output): os.remove(audio_output)\n",
        "\n",
        "            ydl.download([url])\n",
        "\n",
        "        return audio_output\n",
        "\n",
        "def separator_music(input, format, segments_size, overlap, clean_audio, separator_model, backing, reverb):\n",
        "    filename, _ = os.path.splitext(os.path.basename(input))\n",
        "    output = os.path.join(\"audios\", filename)\n",
        "\n",
        "    if not os.path.exists(output): os.makedirs(output)\n",
        "    backing_reverb = backing and reverb\n",
        "\n",
        "    !python main/inference/separator_music.py --input_path $input --output_path $output --format $format --shifts 2 --segments_size $segments_size --overlap $overlap --mdx_hop_length 1024 --mdx_batch_size 1 --clean_audio $clean_audio --clean_strength 0.5 --kara_model 'Version-2' --backing $backing --mdx_denoise True --reverb $reverb --backing_reverb $backing_reverb --model_name $separator_model --sample_rate 44100\n",
        "\n",
        "def is_url(path):\n",
        "    try:\n",
        "        result = urlparse(path)\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "tach_be = False # @param {\"type\":\"boolean\"}\n",
        "tach_vang = False # @param {\"type\":\"boolean\"}\n",
        "duong_dan = \"\" #@param {\"type\":\"string\", \"placeholder\":\"Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp √¢m thanh ho·∫∑c ƒë∆∞·ªùng d·∫´n li√™n k·∫øt √¢m thanh\"}\n",
        "mo_hinh_tach_nhac = \"Voc_FT\" #@param [\"Main_340\", \"Main_390\", \"Main_406\", \"Main_427\", \"Main_438\", \"Inst_full_292\", \"Inst_HQ_1\", \"Inst_HQ_2\", \"Inst_HQ_3\", \"Inst_HQ_4\", \"Inst_HQ_5\", \"Kim_Vocal_1\", \"Kim_Vocal_2\", \"Kim_Inst\", \"Inst_187_beta\", \"Inst_82_beta\", \"Inst_90_beta\", \"Voc_FT\", \"Crowd_HQ\", \"Inst_1\", \"Inst_2\", \"Inst_3\", \"MDXNET_1_9703\", \"MDXNET_2_9682\", \"MDXNET_3_9662\", \"Inst_Main\", \"MDXNET_Main\", \"MDXNET_9482\", \"HT-Normal\", \"HT-Tuned\", \"HD_MMI\",  \"HT_6S\"]\n",
        "chong_cheo = \"0.25\" # @param [\"0.25\", \"0.5\", \"0.75\", \"0.99\"]\n",
        "kich_thuoc_phan_doan = 256 # @param {\"type\":\"slider\",\"min\":32,\"max\":2048,\"step\":8}\n",
        "loc_tap_am = False # @param {\"type\":\"boolean\"}\n",
        "dinh_dang_am_thanh = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "\n",
        "%cd /content/Vietnamese_RVC/\n",
        "\n",
        "if not duong_dan:\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_path = os.path.join(\"audios\", os.path.basename(filename).replace(' ', '_').replace('(', '').replace(')', '').replace('{', '').replace('}', ''))\n",
        "    shutil.move(filename, input_path)\n",
        "else: input_path = download_url(duong_dan) if is_url(duong_dan) else duong_dan\n",
        "\n",
        "separator_music(input_path, dinh_dang_am_thanh, kich_thuoc_phan_doan, chong_cheo, loc_tap_am, mo_hinh_tach_nhac, tach_be, tach_vang)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekfkFFNqppfM"
      },
      "source": [
        "# **Chuy·ªÉn ƒë·ªïi √¢m thanh üîä**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DSMX-pHjb9Zr"
      },
      "outputs": [],
      "source": [
        "#@title **üîé T√¨m ki·∫øm m√¥ h√¨nh**\n",
        "import json\n",
        "import codecs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_models_data(search):\n",
        "    all_table_data = []\n",
        "    page = 1\n",
        "\n",
        "    while 1:\n",
        "        try:\n",
        "            response = requests.post(url=codecs.decode(\"uggcf://ibvpr-zbqryf.pbz/srgpu_qngn.cuc\", \"rot13\"), data={\"page\": page, \"search\": search})\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                table_data = response.json().get(\"table\", \"\")\n",
        "                if not table_data.strip(): break\n",
        "                all_table_data.append(table_data)\n",
        "                page += 1\n",
        "            else: raise Exception(f\"ƒê√£ x·∫£y ra l·ªói, M√£ l·ªói: {response.status_code}\")\n",
        "        except json.JSONDecodeError:\n",
        "            raise Exception(\"ƒê√£ x·∫£y ra l·ªói kh√¥ng th·ªÉ ph√¢n t√≠ch t·ª´ ph·∫£n h·ªìi.\")\n",
        "        except requests.RequestException as e:\n",
        "            raise Exception(f\"G·ª≠i y√™u c·∫ßu th·∫•t b·∫°i: {e}\")\n",
        "    return all_table_data\n",
        "\n",
        "def search_models(name):\n",
        "    tables = fetch_models_data(name)\n",
        "\n",
        "    if len(tables) == 0: print(\"Kh√¥ng t√¨m th·∫•y...\")\n",
        "    else:\n",
        "        for table in tables:\n",
        "            for row in BeautifulSoup(table, \"html.parser\").select(\"tr\"):\n",
        "                name_tag, url_tag = row.find(\"a\", {\"class\": \"fs-5\"}), row.find(\"a\", {\"class\": \"btn btn-sm fw-bold btn-light ms-0 p-1 ps-2 pe-2\"})\n",
        "                if name_tag and url_tag:\n",
        "                    name = name_tag.text.replace(\".pth\", \"\").replace(\".index\", \"\").replace(\".zip\", \"\").replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip()\n",
        "                    url = url_tag[\"href\"].replace(\"https://easyaivoice.com/run?url=\", \"\")\n",
        "                    print(f\"{name}: {url}\")\n",
        "\n",
        "#@markdown **T√¨m ki·∫øm li√™n k·∫øt m√¥ h√¨nh**\n",
        "ten_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n c·ªßa m√¥ h√¨nh c·∫ßn t√¨m\"}\n",
        "search_models(ten_mo_hinh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y7KVxsDcb9mU"
      },
      "outputs": [],
      "source": [
        "#@title **üì© T·∫£i xu·ªëng m√¥ h√¨nh**\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "def move_files_from_directory(src_dir, dest_weights, dest_logs, model_name):\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file.endswith(\".index\"):\n",
        "                model_log_dir = os.path.join(dest_logs, model_name)\n",
        "                os.makedirs(model_log_dir, exist_ok=True)\n",
        "\n",
        "                filepath = os.path.join(model_log_dir, file.replace(' ', '_').replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip())\n",
        "                if os.path.exists(filepath): os.remove(filepath)\n",
        "                shutil.move(file_path, filepath)\n",
        "            elif file.endswith(\".pth\") and not file.startswith(\"D_\") and not file.startswith(\"G_\"):\n",
        "                pth_path = os.path.join(dest_weights, model_name + \".pth\")\n",
        "                if os.path.exists(pth_path): os.remove(pth_path)\n",
        "                shutil.move(file_path, pth_path)\n",
        "\n",
        "def download_model(url=None, model=None):\n",
        "    if not url: raise ValueError(\"H√£y cung c·∫•p ƒë∆∞·ªùng d·∫´n li√™n k·∫øt m√¥ h√¨nh h·ª£p l·ªá!\")\n",
        "    if not model: raise ValueError(\"H√£y cung c·∫•p t√™n m√¥ h√¨nh ƒë·ªÉ t·∫£i v·ªÅ!\")\n",
        "\n",
        "    model = model.replace(\".pth\", \"\").replace(\".index\", \"\").replace(\".zip\", \"\").replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace('\"', \"\").replace(\"'\", \"\").replace(\"|\", \"\").strip()\n",
        "    url = url.replace(\"/blob/\", \"/resolve/\").replace(\"?download=true\", \"\").strip()\n",
        "\n",
        "    download_dir = os.path.join(\"download_model\")\n",
        "    weights_dir = os.path.join(\"assets\", \"weights\")\n",
        "    logs_dir = os.path.join(\"assets\", \"logs\")\n",
        "\n",
        "    if not os.path.exists(download_dir): os.makedirs(download_dir, exist_ok=True)\n",
        "    if not os.path.exists(weights_dir): os.makedirs(weights_dir, exist_ok=True)\n",
        "    if not os.path.exists(logs_dir): os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        from main.tools import gdown, meganz, mediafire, pixeldrain, huggingface\n",
        "\n",
        "        if url.endswith(\".pth\"): huggingface.HF_download_file(url, os.path.join(weights_dir, f\"{model}.pth\"))\n",
        "        elif url.endswith(\".index\"):\n",
        "            model_log_dir = os.path.join(logs_dir, model)\n",
        "            os.makedirs(model_log_dir, exist_ok=True)\n",
        "\n",
        "            huggingface.HF_download_file(url, os.path.join(model_log_dir, f\"{model}.index\"))\n",
        "        elif url.endswith(\".zip\"):\n",
        "            output_path = huggingface.HF_download_file(url, os.path.join(download_dir, model + \".zip\"))\n",
        "            shutil.unpack_archive(output_path, download_dir)\n",
        "\n",
        "            move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "        else:\n",
        "            if \"drive.google.com\" in url or \"drive.usercontent.google.com\" in url:\n",
        "                file_id = None\n",
        "\n",
        "                if \"/file/d/\" in url: file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
        "                elif \"open?id=\" in url: file_id = url.split(\"open?id=\")[1].split(\"/\")[0]\n",
        "                elif \"/download?id=\" in url: file_id = url.split(\"/download?id=\")[1].split(\"&\")[0]\n",
        "\n",
        "                if file_id:\n",
        "                    file = gdown.gdown_download(id=file_id, output=download_dir)\n",
        "                    if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                    move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"mega.nz\" in url:\n",
        "                meganz.mega_download_url(url, download_dir)\n",
        "\n",
        "                file_download = next((f for f in os.listdir(download_dir)), None)\n",
        "                if file_download.endswith(\".zip\"): shutil.unpack_archive(os.path.join(download_dir, file_download), download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"mediafire.com\" in url:\n",
        "                file = mediafire.Mediafire_Download(url, download_dir)\n",
        "                if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            elif \"pixeldrain.com\" in url:\n",
        "                file = pixeldrain.pixeldrain(url, download_dir)\n",
        "                if file.endswith(\".zip\"): shutil.unpack_archive(file, download_dir)\n",
        "\n",
        "                move_files_from_directory(download_dir, weights_dir, logs_dir, model)\n",
        "            else: raise ValueError(\"Li√™n k·∫øt kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£\")\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói: {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(download_dir, ignore_errors=True)\n",
        "\n",
        "def save_drop_model(dropbox):\n",
        "    weight_folder = os.path.join(\"assets\", \"weights\")\n",
        "    logs_folder = os.path.join(\"assets\", \"logs\")\n",
        "    save_model_temp = os.path.join(\"save_model_temp\")\n",
        "\n",
        "    if not os.path.exists(weight_folder): os.makedirs(weight_folder, exist_ok=True)\n",
        "    if not os.path.exists(logs_folder): os.makedirs(logs_folder, exist_ok=True)\n",
        "    if not os.path.exists(save_model_temp): os.makedirs(save_model_temp, exist_ok=True)\n",
        "\n",
        "    shutil.move(dropbox, save_model_temp)\n",
        "\n",
        "    try:\n",
        "        file_name = os.path.basename(dropbox)\n",
        "\n",
        "        if file_name.endswith(\".pth\") and file_name.endswith(\".index\"): raise ValueError(\"T·ªáp b·∫°n v·ª´a t·∫£i l√™n kh√¥ng ph·∫£i m√¥ h√¨nh!\")\n",
        "        else:\n",
        "            if file_name.endswith(\".zip\"):\n",
        "                shutil.unpack_archive(os.path.join(save_model_temp, file_name), save_model_temp)\n",
        "                move_files_from_directory(save_model_temp, weight_folder, logs_folder, file_name.replace(\".zip\", \"\"))\n",
        "            elif file_name.endswith(\".pth\"):\n",
        "                output_file = os.path.join(weight_folder, file_name)\n",
        "                if os.path.exists(output_file): os.remove(output_file)\n",
        "\n",
        "                shutil.move(os.path.join(save_model_temp, file_name), output_file)\n",
        "            elif file_name.endswith(\".index\"):\n",
        "                def extract_name_model(filename):\n",
        "                    match = re.search(r\"([A-Za-z]+)(?=_v|\\.|$)\", filename)\n",
        "                    return match.group(1) if match else None\n",
        "\n",
        "                model_logs = os.path.join(logs_folder, extract_name_model(file_name))\n",
        "                if not os.path.exists(model_logs): os.makedirs(model_logs, exist_ok=True)\n",
        "                shutil.move(os.path.join(save_model_temp, file_name), model_logs)\n",
        "            else: raise Exception(\"Kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c m√¥ h√¨nh!\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ƒê√£ x·∫£y ra l·ªói {e}\")\n",
        "    finally:\n",
        "        shutil.rmtree(save_model_temp, ignore_errors=True)\n",
        "\n",
        "#@markdown **H·ªó tr·ª£ c√°c li√™n k·∫øt ƒë·∫øn t·ª´ huggingface.co / drive.google.com / mega.nz / mediafire.com**\n",
        "\n",
        "ten_cua_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "lien_ket_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"https://huggingface.co//...\"}\n",
        "\n",
        "if not lien_ket_mo_hinh:\n",
        "    uploaded = files.upload()\n",
        "    save_drop_model(list(uploaded.keys())[0])\n",
        "else:\n",
        "    %cd /content/Vietnamese_RVC\n",
        "\n",
        "    download_model(lien_ket_mo_hinh, ten_cua_mo_hinh)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n t·∫•t!!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fma6vLlab93L",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üéµ **Chuy·ªÉn ƒê·ªïi √Çm Thanh** üéµ\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from ipywidgets import Button\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "from main.library.utils import pydub_convert\n",
        "\n",
        "def convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0_method, input_path, output_path, pth_path, index_path, f0_autotune, clean_audio, clean_strength, export_format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing):\n",
        "    !python main/inference/convert.py --pitch $pitch --filter_radius $filter_radius --index_rate $index_rate --volume_envelope $volume_envelope --protect $protect --hop_length $hop_length --f0_method $f0_method --input_path $input_path --output_path $output_path --pth_path $pth_path --index_path $index_path --f0_autotune $f0_autotune --clean_audio $clean_audio --clean_strength $clean_strength --export_format $export_format --embedder_model $embedder_model --resample_sr $resample_sr --split_audio $split_audio --f0_autotune_strength $f0_autotune_strength --checkpointing $checkpointing\n",
        "\n",
        "def convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, input, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, input_audio_name, checkpointing, apply_effect):\n",
        "    if use_original:\n",
        "        if convert_backing: raise ValueError(\"T·∫Øt chuy·ªÉn ƒë·ªïi gi·ªçng b√® ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng chuy·ªÉn ƒë·ªïi gi·ªçng g·ªëc!\")\n",
        "        elif not_merge_backing: raise ValueError(\"T·∫Øt k·∫øt h·ª£p b√® ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng chuy·ªÉn ƒë·ªïi gi·ªçng g·ªëc!\")\n",
        "\n",
        "    if not model or not os.path.exists(model) or os.path.isdir(model) or not model.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p t·ªáp m√¥ h√¨nh h·ª£p l·ªá!\")\n",
        "\n",
        "    f0method = method if method != \"hybrid\" else hybrid_method\n",
        "    embedder_model = embedders if embedders != \"custom\" else custom_embedders\n",
        "\n",
        "    if use_audio:\n",
        "        output_audio = os.path.join(\"audios\", input_audio_name)\n",
        "\n",
        "        def get_audio_file(label):\n",
        "            matching_files = [f for f in os.listdir(output_audio) if label in f]\n",
        "\n",
        "            if not matching_files: return None\n",
        "            return os.path.join(output_audio, matching_files[0])\n",
        "\n",
        "        output_path = os.path.join(output_audio, f\"Convert_Vocals.{format}\")\n",
        "        output_path_effect = os.path.join(output_audio, f\"Convert_Vocal_Effects.{format}\")\n",
        "        output_backing = os.path.join(output_audio, f\"Convert_Backing.{format}\")\n",
        "        output_merge_backup = os.path.join(output_audio, f\"Vocals+Backing.{format}\")\n",
        "        output_merge_instrument = os.path.join(output_audio, f\"Vocals+Instruments.{format}\")\n",
        "\n",
        "        if os.path.exists(output_audio): os.makedirs(output_audio, exist_ok=True)\n",
        "        if os.path.exists(output_path): os.remove(output_path)\n",
        "\n",
        "        if use_original:\n",
        "            original_vocal = get_audio_file('Original_Vocals_No_Reverb.')\n",
        "\n",
        "            if original_vocal is None: original_vocal = get_audio_file('Original_Vocals.')\n",
        "            if original_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng g·ªëc!\")\n",
        "\n",
        "            input_path = original_vocal\n",
        "        else:\n",
        "            main_vocal = get_audio_file('Main_Vocals_No_Reverb.')\n",
        "            backing_vocal = get_audio_file('Backing_Vocals_No_Reverb.')\n",
        "\n",
        "            if main_vocal is None: main_vocal = get_audio_file('Main_Vocals.')\n",
        "            if not not_merge_backing and backing_vocal is None: backing_vocal = get_audio_file('Backing_Vocals.')\n",
        "\n",
        "            if main_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng ch√≠nh!\")\n",
        "            if not not_merge_backing and backing_vocal is None: raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y gi·ªçng b√®\")\n",
        "\n",
        "            input_path = main_vocal\n",
        "            backing_path = backing_vocal\n",
        "\n",
        "        print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi gi·ªçng...\")\n",
        "\n",
        "        convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input_path, output_path, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "        if apply_effect:\n",
        "            audio_effects(output_path, output_path_effect, format)\n",
        "            output_path = output_path_effect\n",
        "\n",
        "        print(\"Chuy·ªÉn ƒë·ªïi gi·ªçng ho√†n t·∫•t!\")\n",
        "\n",
        "        if convert_backing:\n",
        "            if os.path.exists(output_backing): os.remove(output_backing)\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi gi·ªçng b√®...\")\n",
        "\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, backing_path, output_backing, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "            print(\"Chuy·ªÉn ƒë·ªïi gi·ªçng b√® ho√†n t·∫•t!\")\n",
        "\n",
        "        if not not_merge_backing and not use_original:\n",
        "            backing_source = output_backing if convert_backing else backing_vocal\n",
        "\n",
        "            if os.path.exists(output_merge_backup): os.remove(output_merge_backup)\n",
        "\n",
        "            pydub_convert(AudioSegment.from_file(output_path)).overlay(pydub_convert(AudioSegment.from_file(backing_source))).export(output_merge_backup, format=format)\n",
        "\n",
        "        if merge_instrument:\n",
        "            vocals = output_merge_backup if not not_merge_backing and not use_original else output_path\n",
        "\n",
        "            if os.path.exists(output_merge_instrument): os.remove(output_merge_instrument)\n",
        "\n",
        "            instruments = get_audio_file('Instruments.')\n",
        "\n",
        "            if instruments is None: output_merge_instrument = None\n",
        "            else: pydub_convert(AudioSegment.from_file(instruments)).overlay(pydub_convert(AudioSegment.from_file(vocals))).export(output_merge_instrument, format=format)\n",
        "    else:\n",
        "        if not input or not os.path.exists(input): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "        if not output: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "\n",
        "        if os.path.isdir(input):\n",
        "            print(\"ƒê∆∞·ªùng d·∫´n ƒë·∫ßu v√†o l√† th∆∞ m·ª•c, chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t ƒë∆∞·ª£c k√≠ch ho·∫°t...\")\n",
        "            if not [f for f in os.listdir(input) if f.lower().endswith((\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"))]: raise ValueError(\"Th∆∞ m·ª•c ƒë·∫ßu v√†o tr·ªëng!\")\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t...\")\n",
        "            output_dir = os.path.dirname(output) or output\n",
        "\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input, output_dir, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "            print(\"Chuy·ªÉn ƒë·ªïi h√†ng lo·∫°t ƒë√£ ho√†n t·∫•t!\")\n",
        "        else:\n",
        "            output_dir = os.path.dirname(output) or output\n",
        "\n",
        "            if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "            if os.path.exists(output): os.remove(output)\n",
        "\n",
        "            print(\"B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi t·ªáp ƒë·∫ßu v√†o...\")\n",
        "            convert(pitch, filter_radius, index_rate, volume_envelope, protect, hop_length, f0method, input, output, model, index, autotune, clean, clean_strength, format, embedder_model, resample_sr, split_audio, f0_autotune_strength, checkpointing)\n",
        "\n",
        "            if apply_effect:\n",
        "                filename, _ = os.path.splitext(os.path.basename(output))\n",
        "                audio_effects(output, os.path.join(output_dir, f\"{filename}_effect.{format}\"), format)\n",
        "\n",
        "            print(\"Ho√†n t·∫•t chuy·ªÉn ƒë·ªïi!\")\n",
        "\n",
        "def convert_selection(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, inputp, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, checkpointing, apply_effect):\n",
        "    if use_audio:\n",
        "        choice = [f for f in os.listdir(\"audios\") if os.path.isdir(os.path.join(\"audios\", f))]\n",
        "\n",
        "        if len(choice) == 0:  print(\"Kh√¥ng t√¨m th·∫•y b·∫£n t√°ch nh·∫°c n√†o!\")\n",
        "        elif len(choice) == 1: convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, None, None, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, choice[0], checkpointing, apply_effect)\n",
        "        else:\n",
        "            print(\"ƒê√£ t√¨m th·∫•y nhi·ªÅu h∆°n 1 b·∫£n t√°ch nh·∫°c, vui l√≤ng ch·ªçn b√†i ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "            for c in choice:\n",
        "                print(f\"{choice.index(c)}. {c}\")\n",
        "\n",
        "            while 1:\n",
        "                try:\n",
        "                    choice_select = int(input(\"H√£y nh·∫≠p s·ªë th·ª© t·ª± c·ªßa b·∫£n t√°ch: \"))\n",
        "\n",
        "                    if choice_select < len(choice):\n",
        "                        choice_audio = choice[choice_select]\n",
        "                        print(f\"B·∫°n ƒë√£ ch·ªçn b·∫£n t√°ch: {choice_audio}\")\n",
        "                        break\n",
        "                    else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "                except ValueError:\n",
        "                    print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "                except IndexError:\n",
        "                    print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "            convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, None, None, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, choice_audio, checkpointing, apply_effect)\n",
        "    else: convert_audio(clean, autotune, use_audio, use_original, convert_backing, not_merge_backing, merge_instrument, pitch, clean_strength, model, index, index_rate, inputp, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, None, checkpointing, apply_effect)\n",
        "\n",
        "def audio_effects(input_path, output_path, export_format):\n",
        "    if not input_path or not os.path.exists(input_path) or os.path.isdir(input_path): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "    if not output_path: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "    output_dir = os.path.dirname(output_path) or output_path\n",
        "\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "    if os.path.exists(output_path): os.remove(output_path)\n",
        "\n",
        "    !python main/inference/audio_effects.py --input_path $input_path --output_path $output_path --reverb_room_size 0.15 --reverb_damping 0.7 --reverb_wet_level 0.2 --reverb_dry_level 0.8 --reverb_width 1.0 --reverb_freeze_mode False --export_format $export_format --reverb True\n",
        "\n",
        "def get_index(model):\n",
        "    return next((f for f in [os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\") and \"trained\" not in name] if model.split(\".\")[0] in f), \"\")\n",
        "\n",
        "su_dung_giong_goc = False #@param {\"type\":\"boolean\"}\n",
        "chuyen_doi_giong_be = False #@param {\"type\":\"boolean\"}\n",
        "ket_hop_nhac_nen = False #@param {\"type\":\"boolean\"}\n",
        "cao_do = 0 #@param {\"type\":\"slider\",\"min\":-20,\"max\":20,\"step\":1}\n",
        "ten_mo_hinh = \"\" #@param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "anh_huong_cua_chi_muc = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "duong_dan_dau_vao = \"\" #@param {\"type\":\"string\",\"placeholder\":\"ƒê∆∞·ªùng d·∫´n t·ªáp √¢m thanh\"}\n",
        "dinh_dang_tep = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "phuong_phap_trich_xuat = \"rmvpe\" #@param [\"pm\", \"dio\", \"mangio-crepe-tiny\", \"mangio-crepe-tiny-onnx\", \"mangio-crepe-small\", \"mangio-crepe-small-onnx\", \"mangio-crepe-medium\", \"mangio-crepe-medium-onnx\", \"mangio-crepe-large\", \"mangio-crepe-large-onnx\", \"mangio-crepe-full\", \"mangio-crepe-full-onnx\", \"crepe-tiny\", \"crepe-tiny-onnx\", \"crepe-small\", \"crepe-small-onnx\", \"crepe-medium\", \"crepe-medium-onnx\", \"crepe-large\", \"crepe-large-onnx\", \"crepe-full\", \"crepe-full-onnx\", \"fcpe\", \"fcpe-onnx\", \"fcpe-legacy\", \"fcpe-legacy-onnx\", \"rmvpe\", \"rmvpe-onnx\", \"rmvpe-legacy\", \"rmvpe-legacy-onnx\", \"harvest\", \"yin\", \"pyin\"]\n",
        "hop_length = 64 # @param {\"type\":\"slider\",\"min\":64,\"max\":512,\"step\":1}\n",
        "bao_ve_phu_am = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.1}\n",
        "loc_tap_am = False #@param {\"type\":\"boolean\"}\n",
        "tu_dong_dieu_chinh = False #@param {\"type\":\"boolean\"}\n",
        "cat_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "ap_dung_hieu_ung = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if ten_mo_hinh:\n",
        "    model_path = os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}.pth\")\n",
        "    index_path = get_index(f\"{ten_mo_hinh}.pth\".split(\"_\")[0])\n",
        "else:\n",
        "    model_name = sorted(list(model for model in os.listdir(os.path.join(\"assets\", \"weights\")) if model.endswith(\".pth\") and not model.startswith(\"G_\") and not model.startswith(\"D_\")))\n",
        "    indexpath = sorted([os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\")])\n",
        "\n",
        "    if len(model_name) < 1: raise ValueError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "    elif len(model_name) == 1:\n",
        "        model_path = os.path.join(\"assets\", \"weights\", model_name[0])\n",
        "        index_path = get_index(os.path.basename(model_name[0])[0])\n",
        "    else:\n",
        "        print(\"T√¨m th·∫•y nhi·ªÅu h∆°n 1 m√¥ h√¨nh, vui l√≤ng nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "        for m in model_name:\n",
        "            print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "                if model_index < len(model_name):\n",
        "                    selected_model = model_name[model_index]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        model_path = os.path.join(\"assets\", \"weights\", selected_model)\n",
        "        index_path = get_index(os.path.basename(selected_model).split(\"_\")[0])\n",
        "\n",
        "if not index_path: print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c!\")\n",
        "\n",
        "if not duong_dan_dau_vao and not (su_dung_giong_goc or chuyen_doi_giong_be or ket_hop_nhac_nen):\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_path = os.path.join(\"audios\", os.path.basename(filename).replace(' ', '_').replace('(', '').replace(')', '').replace('{', '').replace('}', ''))\n",
        "    shutil.move(filename, input_path)\n",
        "else: input_path = duong_dan_dau_vao\n",
        "\n",
        "convert_selection(loc_tap_am, tu_dong_dieu_chinh, (su_dung_giong_goc or chuyen_doi_giong_be or ket_hop_nhac_nen), su_dung_giong_goc, chuyen_doi_giong_be, False, ket_hop_nhac_nen, cao_do, 0.7, model_path, index_path, anh_huong_cua_chi_muc, input_path, os.path.join(\"audios\", \"output.wav\"), dinh_dang_tep, phuong_phap_trich_xuat, None, hop_length, \"contentvec_base\", None, 0, 3, 1, bao_ve_phu_am, cat_am_thanh, 1, False, ap_dung_hieu_ung)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEBxigvvb-Ds",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üéµ **Chuy·ªÉn ƒê·ªïi VƒÉn B·∫£n** üéµ\n",
        "import os\n",
        "\n",
        "from ipywidgets import Button\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "%cd /content/Vietnamese_RVC\n",
        "\n",
        "from main.tools import edge_tts\n",
        "\n",
        "async def TTS(prompt, voice, speed, output, pitch):\n",
        "    if not prompt: raise ValueError(\"Vui l√≤ng cung c·∫•p vƒÉn b·∫£n ƒë·ªÉ ƒë·ªçc!\")\n",
        "    if not voice: raise ValueError(\"Vui l√≤ng ch·ªçn gi·ªçng n√≥i ƒë·ªÉ chuy·ªÉn ƒë·ªïi!\")\n",
        "    if not output: raise ValueError(\"Vui l√≤ng cung c·∫•p ƒë·∫ßu v√†o h·ª£p l·ªá!\")\n",
        "\n",
        "    if os.path.isdir(output): output = os.path.join(output, f\"tts.wav\")\n",
        "\n",
        "    output_dir = os.path.dirname(output) or output\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    await edge_tts.Communicate(text=prompt, voice=voice, rate=f\"+{speed}%\" if speed >= 0 else f\"{speed}%\", pitch=f\"+{pitch}Hz\" if pitch >= 0 else f\"{pitch}Hz\").save(output)\n",
        "    return output\n",
        "\n",
        "def convert_tts(clean, autotune, pitch, clean_strength, model, index, index_rate, input, output, format, method, hybrid_method, hop_length, embedders, custom_embedders, resample_sr, filter_radius, volume_envelope, protect, split_audio, f0_autotune_strength, checkpointing):\n",
        "    if not model or not os.path.exists(model) or os.path.isdir(model) or not model.endswith(\".pth\"): raise FileExistsError(\"Vui l√≤ng cung c·∫•p t·ªáp m√¥ h√¨nh h·ª£p l·ªá\")\n",
        "    if not input or not os.path.exists(input): raise FileNotFoundError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá!\")\n",
        "    if not output: raise ValueError(\"ƒê∆∞·ªùng d·∫´n t·ªáp ƒë·∫ßu ra kh√¥ng h·ª£p l·ªá!\")\n",
        "\n",
        "    if os.path.isdir(output): output = os.path.join(output, f\"tts.{format}\")\n",
        "\n",
        "    output_dir = os.path.dirname(output)\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(output): os.remove(output)\n",
        "\n",
        "    f0method = method if method != \"hybrid\" else hybrid_method\n",
        "    embedder_model = embedders if embedders != \"custom\" else custom_embedders\n",
        "\n",
        "    !python main/inference/convert.py --pitch $pitch --filter_radius $filter_radius --index_rate $index_rate --volume_envelope $volume_envelope --protect $protect --hop_length $hop_length --f0_method $f0method --input_path $input --output_path $output --pth_path $model --index_path $index --f0_autotune $autotune --clean_audio $clean --clean_strength $clean_strength --export_format $format --embedder_model $embedder_model --resample_sr $resample_sr --split_audio $split_audio --f0_autotune_strength $f0_autotune_strength --checkpointing $checkpointing\n",
        "\n",
        "def get_index(model):\n",
        "    return next((f for f in [os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\") and \"trained\" not in name] if model.split(\".\")[0] in f), \"\")\n",
        "\n",
        "def process_input(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        file_contents = file.read()\n",
        "\n",
        "    return file_contents\n",
        "\n",
        "van_ban = \"\" # @param {\"type\":\"string\",\"placeholder\":\"VƒÉn b·∫£n c·∫ßn ƒë·ªçc\"}\n",
        "giong_noi = \"vi-VN-NamMinhNeural\" # @param ['af-ZA-AdriNeural', 'af-ZA-WillemNeural', 'sq-AL-AnilaNeural', 'sq-AL-IlirNeural', 'am-ET-AmehaNeural', 'am-ET-MekdesNeural', 'ar-DZ-AminaNeural', 'ar-DZ-IsmaelNeural', 'ar-BH-AliNeural', 'ar-BH-LailaNeural', 'ar-EG-SalmaNeural', 'ar-EG-ShakirNeural', 'ar-IQ-BasselNeural', 'ar-IQ-RanaNeural', 'ar-JO-SanaNeural', 'ar-JO-TaimNeural', 'ar-KW-FahedNeural', 'ar-KW-NouraNeural', 'ar-LB-LaylaNeural', 'ar-LB-RamiNeural', 'ar-LY-ImanNeural', 'ar-LY-OmarNeural', 'ar-MA-JamalNeural', 'ar-MA-MounaNeural', 'ar-OM-AbdullahNeural', 'ar-OM-AyshaNeural', 'ar-QA-AmalNeural', 'ar-QA-MoazNeural', 'ar-SA-HamedNeural', 'ar-SA-ZariyahNeural', 'ar-SY-AmanyNeural', 'ar-SY-LaithNeural', 'ar-TN-HediNeural', 'ar-TN-ReemNeural', 'ar-AE-FatimaNeural', 'ar-AE-HamdanNeural', 'ar-YE-MaryamNeural', 'ar-YE-SalehNeural', 'az-AZ-BabekNeural', 'az-AZ-BanuNeural', 'bn-BD-NabanitaNeural', 'bn-BD-PradeepNeural', 'bn-IN-BashkarNeural', 'bn-IN-TanishaaNeural', 'bs-BA-GoranNeural', 'bs-BA-VesnaNeural', 'bg-BG-BorislavNeural', 'bg-BG-KalinaNeural', 'my-MM-NilarNeural', 'my-MM-ThihaNeural', 'ca-ES-EnricNeural', 'ca-ES-JoanaNeural', 'zh-HK-HiuGaaiNeural', 'zh-HK-HiuMaanNeural', 'zh-HK-WanLungNeural', 'zh-CN-XiaoxiaoNeural', 'zh-CN-XiaoyiNeural', 'zh-CN-YunjianNeural', 'zh-CN-YunxiNeural', 'zh-CN-YunxiaNeural', 'zh-CN-YunyangNeural', 'zh-CN-liaoning-XiaobeiNeural', 'zh-TW-HsiaoChenNeural', 'zh-TW-YunJheNeural', 'zh-TW-HsiaoYuNeural', 'zh-CN-shaanxi-XiaoniNeural', 'hr-HR-GabrijelaNeural', 'hr-HR-SreckoNeural', 'cs-CZ-AntoninNeural', 'cs-CZ-VlastaNeural', 'da-DK-ChristelNeural', 'da-DK-JeppeNeural', 'nl-BE-ArnaudNeural', 'nl-BE-DenaNeural', 'nl-NL-ColetteNeural', 'nl-NL-FennaNeural', 'nl-NL-MaartenNeural', 'en-AU-NatashaNeural', 'en-AU-WilliamNeural', 'en-CA-ClaraNeural', 'en-CA-LiamNeural', 'en-HK-SamNeural', 'en-HK-YanNeural', 'en-IN-NeerjaExpressiveNeural', 'en-IN-NeerjaNeural', 'en-IN-PrabhatNeural', 'en-IE-ConnorNeural', 'en-IE-EmilyNeural', 'en-KE-AsiliaNeural', 'en-KE-ChilembaNeural', 'en-NZ-MitchellNeural', 'en-NZ-MollyNeural', 'en-NG-AbeoNeural', 'en-NG-EzinneNeural', 'en-PH-JamesNeural', 'en-PH-RosaNeural', 'en-SG-LunaNeural', 'en-SG-WayneNeural', 'en-ZA-LeahNeural', 'en-ZA-LukeNeural', 'en-TZ-ElimuNeural', 'en-TZ-ImaniNeural', 'en-GB-LibbyNeural', 'en-GB-MaisieNeural', 'en-GB-RyanNeural', 'en-GB-SoniaNeural', 'en-GB-ThomasNeural', 'en-US-AvaMultilingualNeural', 'en-US-AndrewMultilingualNeural', 'en-US-EmmaMultilingualNeural', 'en-US-BrianMultilingualNeural', 'en-US-AvaNeural', 'en-US-AndrewNeural', 'en-US-EmmaNeural', 'en-US-BrianNeural', 'en-US-AnaNeural', 'en-US-AriaNeural', 'en-US-ChristopherNeural', 'en-US-EricNeural', 'en-US-GuyNeural', 'en-US-JennyNeural', 'en-US-MichelleNeural', 'en-US-RogerNeural', 'en-US-SteffanNeural', 'et-EE-AnuNeural', 'et-EE-KertNeural', 'fil-PH-AngeloNeural', 'fil-PH-BlessicaNeural', 'fi-FI-HarriNeural', 'fi-FI-NooraNeural', 'fr-BE-CharlineNeural', 'fr-BE-GerardNeural', 'fr-CA-ThierryNeural', 'fr-CA-AntoineNeural', 'fr-CA-JeanNeural', 'fr-CA-SylvieNeural', 'fr-FR-VivienneMultilingualNeural', 'fr-FR-RemyMultilingualNeural', 'fr-FR-DeniseNeural', 'fr-FR-EloiseNeural', 'fr-FR-HenriNeural', 'fr-CH-ArianeNeural', 'fr-CH-FabriceNeural', 'gl-ES-RoiNeural', 'gl-ES-SabelaNeural', 'ka-GE-EkaNeural', 'ka-GE-GiorgiNeural', 'de-AT-IngridNeural', 'de-AT-JonasNeural', 'de-DE-SeraphinaMultilingualNeural', 'de-DE-FlorianMultilingualNeural', 'de-DE-AmalaNeural', 'de-DE-ConradNeural', 'de-DE-KatjaNeural', 'de-DE-KillianNeural', 'de-CH-JanNeural', 'de-CH-LeniNeural', 'el-GR-AthinaNeural', 'el-GR-NestorasNeural', 'gu-IN-DhwaniNeural', 'gu-IN-NiranjanNeural', 'he-IL-AvriNeural', 'he-IL-HilaNeural', 'hi-IN-MadhurNeural', 'hi-IN-SwaraNeural', 'hu-HU-NoemiNeural', 'hu-HU-TamasNeural', 'is-IS-GudrunNeural', 'is-IS-GunnarNeural', 'id-ID-ArdiNeural', 'id-ID-GadisNeural', 'ga-IE-ColmNeural', 'ga-IE-OrlaNeural', 'it-IT-GiuseppeNeural', 'it-IT-DiegoNeural', 'it-IT-ElsaNeural', 'it-IT-IsabellaNeural', 'ja-JP-KeitaNeural', 'ja-JP-NanamiNeural', 'jv-ID-DimasNeural', 'jv-ID-SitiNeural', 'kn-IN-GaganNeural', 'kn-IN-SapnaNeural', 'kk-KZ-AigulNeural', 'kk-KZ-DauletNeural', 'km-KH-PisethNeural', 'km-KH-SreymomNeural', 'ko-KR-HyunsuNeural', 'ko-KR-InJoonNeural', 'ko-KR-SunHiNeural', 'lo-LA-ChanthavongNeural', 'lo-LA-KeomanyNeural', 'lv-LV-EveritaNeural', 'lv-LV-NilsNeural', 'lt-LT-LeonasNeural', 'lt-LT-OnaNeural', 'mk-MK-AleksandarNeural', 'mk-MK-MarijaNeural', 'ms-MY-OsmanNeural', 'ms-MY-YasminNeural', 'ml-IN-MidhunNeural', 'ml-IN-SobhanaNeural', 'mt-MT-GraceNeural', 'mt-MT-JosephNeural', 'mr-IN-AarohiNeural', 'mr-IN-ManoharNeural', 'mn-MN-BataaNeural', 'mn-MN-YesuiNeural', 'ne-NP-HemkalaNeural', 'ne-NP-SagarNeural', 'nb-NO-FinnNeural', 'nb-NO-PernilleNeural', 'ps-AF-GulNawazNeural', 'ps-AF-LatifaNeural', 'fa-IR-DilaraNeural', 'fa-IR-FaridNeural', 'pl-PL-MarekNeural', 'pl-PL-ZofiaNeural', 'pt-BR-ThalitaNeural', 'pt-BR-AntonioNeural', 'pt-BR-FranciscaNeural', 'pt-PT-DuarteNeural', 'pt-PT-RaquelNeural', 'ro-RO-AlinaNeural', 'ro-RO-EmilNeural', 'ru-RU-DmitryNeural', 'ru-RU-SvetlanaNeural', 'sr-RS-NicholasNeural', 'sr-RS-SophieNeural', 'si-LK-SameeraNeural', 'si-LK-ThiliniNeural', 'sk-SK-LukasNeural', 'sk-SK-ViktoriaNeural', 'sl-SI-PetraNeural', 'sl-SI-RokNeural', 'so-SO-MuuseNeural', 'so-SO-UbaxNeural', 'es-AR-ElenaNeural', 'es-AR-TomasNeural', 'es-BO-MarceloNeural', 'es-BO-SofiaNeural', 'es-CL-CatalinaNeural', 'es-CL-LorenzoNeural', 'es-ES-XimenaNeural', 'es-CO-GonzaloNeural', 'es-CO-SalomeNeural', 'es-CR-JuanNeural', 'es-CR-MariaNeural', 'es-CU-BelkysNeural', 'es-CU-ManuelNeural', 'es-DO-EmilioNeural', 'es-DO-RamonaNeural', 'es-EC-AndreaNeural', 'es-EC-LuisNeural', 'es-SV-LorenaNeural', 'es-SV-RodrigoNeural', 'es-GQ-JavierNeural', 'es-GQ-TeresaNeural', 'es-GT-AndresNeural', 'es-GT-MartaNeural', 'es-HN-CarlosNeural', 'es-HN-KarlaNeural', 'es-MX-DaliaNeural', 'es-MX-JorgeNeural', 'es-NI-FedericoNeural', 'es-NI-YolandaNeural', 'es-PA-MargaritaNeural', 'es-PA-RobertoNeural', 'es-PY-MarioNeural', 'es-PY-TaniaNeural', 'es-PE-AlexNeural', 'es-PE-CamilaNeural', 'es-PR-KarinaNeural', 'es-PR-VictorNeural', 'es-ES-AlvaroNeural', 'es-ES-ElviraNeural', 'es-US-AlonsoNeural', 'es-US-PalomaNeural', 'es-UY-MateoNeural', 'es-UY-ValentinaNeural', 'es-VE-PaolaNeural', 'es-VE-SebastianNeural', 'su-ID-JajangNeural', 'su-ID-TutiNeural', 'sw-KE-RafikiNeural', 'sw-KE-ZuriNeural', 'sw-TZ-DaudiNeural', 'sw-TZ-RehemaNeural', 'sv-SE-MattiasNeural', 'sv-SE-SofieNeural', 'ta-IN-PallaviNeural', 'ta-IN-ValluvarNeural', 'ta-MY-KaniNeural', 'ta-MY-SuryaNeural', 'ta-SG-AnbuNeural', 'ta-SG-VenbaNeural', 'ta-LK-KumarNeural', 'ta-LK-SaranyaNeural', 'te-IN-MohanNeural', 'te-IN-ShrutiNeural', 'th-TH-NiwatNeural', 'th-TH-PremwadeeNeural', 'tr-TR-AhmetNeural', 'tr-TR-EmelNeural', 'uk-UA-OstapNeural', 'uk-UA-PolinaNeural', 'ur-IN-GulNeural', 'ur-IN-SalmanNeural', 'ur-PK-AsadNeural', 'ur-PK-UzmaNeural', 'uz-UZ-MadinaNeural', 'uz-UZ-SardorNeural', 'vi-VN-HoaiMyNeural', 'vi-VN-NamMinhNeural', 'cy-GB-AledNeural', 'cy-GB-NiaNeural', 'zu-ZA-ThandoNeural', 'zu-ZA-ThembaNeural']\n",
        "toc_do_doc = 0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":1}\n",
        "\n",
        "cao_do = 0 #@param {\"type\":\"slider\",\"min\":-20,\"max\":20,\"step\":1}\n",
        "ten_mo_hinh = \"\" #@param {\"type\":\"string\",\"placeholder\":\"T√™n m√¥ h√¨nh\"}\n",
        "anh_huong_cua_chi_muc = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "dinh_dang_tep = \"wav\" #@param [\"wav\", \"mp3\", \"flac\", \"ogg\", \"opus\", \"m4a\", \"mp4\", \"aac\", \"alac\", \"wma\", \"aiff\", \"webm\", \"ac3\"]\n",
        "phuong_phap_trich_xuat = \"rmvpe\" #@param [\"pm\", \"dio\", \"mangio-crepe-tiny\", \"mangio-crepe-tiny-onnx\", \"mangio-crepe-small\", \"mangio-crepe-small-onnx\", \"mangio-crepe-medium\", \"mangio-crepe-medium-onnx\", \"mangio-crepe-large\", \"mangio-crepe-large-onnx\", \"mangio-crepe-full\", \"mangio-crepe-full-onnx\", \"crepe-tiny\", \"crepe-tiny-onnx\", \"crepe-small\", \"crepe-small-onnx\", \"crepe-medium\", \"crepe-medium-onnx\", \"crepe-large\", \"crepe-large-onnx\", \"crepe-full\", \"crepe-full-onnx\", \"fcpe\", \"fcpe-onnx\", \"fcpe-legacy\", \"fcpe-legacy-onnx\", \"rmvpe\", \"rmvpe-onnx\", \"rmvpe-legacy\", \"rmvpe-legacy-onnx\", \"harvest\", \"yin\", \"pyin\"]\n",
        "hop_length = 64 # @param {\"type\":\"slider\",\"min\":64,\"max\":512,\"step\":1}\n",
        "bao_ve_phu_am = 0.5 #@param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.1}\n",
        "loc_tap_am = False #@param {\"type\":\"boolean\"}\n",
        "tu_dong_dieu_chinh = False #@param {\"type\":\"boolean\"}\n",
        "cat_am_thanh = False #@param {\"type\":\"boolean\"}\n",
        "\n",
        "if van_ban: input_text = van_ban\n",
        "else:\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    input_text = process_input(filename)\n",
        "\n",
        "if ten_mo_hinh:\n",
        "    model_path = os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}.pth\")\n",
        "    index_path = get_index(f\"{ten_mo_hinh}.pth\".split(\"_\")[0])\n",
        "else:\n",
        "    model_name = sorted(list(model for model in os.listdir(os.path.join(\"assets\", \"weights\")) if model.endswith(\".pth\") and not model.startswith(\"G_\") and not model.startswith(\"D_\")))\n",
        "    indexpath = sorted([os.path.join(root, name) for root, _, files in os.walk(os.path.join(\"assets\", \"logs\"), topdown=False) for name in files if name.endswith(\".index\")])\n",
        "\n",
        "    if len(model_name) < 1: raise ValueError(\"Vui l√≤ng cung c·∫•p m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "    elif len(model_name) == 1:\n",
        "        model_path = os.path.join(\"assets\", \"weights\", model_name[0])\n",
        "        index_path = get_index(os.path.basename(model_name[0])[0])\n",
        "    else:\n",
        "        print(\"T√¨m th·∫•y nhi·ªÅu h∆°n 1 m√¥ h√¨nh, vui l√≤ng nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh ƒë·ªÉ ti·∫øn h√†nh chuy·ªÉn ƒë·ªïi!\")\n",
        "        for m in model_name:\n",
        "            print(f\"{model_name.index(m)}. {m}\")\n",
        "\n",
        "        while 1:\n",
        "            try:\n",
        "                model_index = int(input(\"Nh·∫≠p s·ªë th·ª© t·ª± c·ªßa m√¥ h√¨nh: \"))\n",
        "                if model_index < len(model_name):\n",
        "                    selected_model = model_name[model_index]\n",
        "                    print(f\"B·∫°n ƒë√£ ch·ªçn m√¥ h√¨nh: {selected_model}\")\n",
        "                    break\n",
        "                else: print(\"S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "            except ValueError:\n",
        "                print(\"Vui l√≤ng nh·∫≠p m·ªôt s·ªë nguy√™n.\")\n",
        "            except IndexError:\n",
        "                print(\"S·ªë th·ª© t·ª± v∆∞·ª£t qu√° ph·∫°m vi. Vui l√≤ng nh·∫≠p l·∫°i.\")\n",
        "\n",
        "        model_path = os.path.join(\"assets\", \"weights\", selected_model)\n",
        "        index_path = get_index(os.path.basename(selected_model).split(\"_\")[0])\n",
        "\n",
        "if not index_path: print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c!\")\n",
        "\n",
        "convert_tts(loc_tap_am, tu_dong_dieu_chinh, cao_do, 0.7, model_path, index_path, anh_huong_cua_chi_muc, await TTS(input_text, giong_noi, toc_do_doc, os.path.join(\"audios\", f\"tts.{dinh_dang_tep}\"), 0), os.path.join(\"audios\", f\"tts-convert.{dinh_dang_tep}\"), dinh_dang_tep, phuong_phap_trich_xuat, None, hop_length, \"contentvec_base\", None, 0, 3, 1, bao_ve_phu_am, cat_am_thanh, 1, False)\n",
        "\n",
        "clear_output()\n",
        "display(Button(description=\"\\u2714 Ho√†n T·∫•t!\", button_style=\"success\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ers351v_CMGN"
      },
      "source": [
        "# **Hu·∫•n luy·ªán m√¥ h√¨nh ü§ñ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqzzBxoK6DeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgNk1aArb_h8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLJHMJU5b_lz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4yZIXYyCqZL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **N√©n v√† l∆∞u m√¥ h√¨nh üì¶**\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "\n",
        "from ipywidgets import Button\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#@markdown **T√™n c·ªßa m√¥ h√¨nh c·∫ßn n√©n**\n",
        "ten_mo_hinh = \"\" # @param {\"type\":\"string\",\"placeholder\":\"My_Model\"}\n",
        "#@markdown **L·∫•y m√¥ h√¨nh t·ªët nh·∫•t khi s·ª≠ d·ª•ng ki·ªÉm tra hu·∫•n luy·ªán**\n",
        "mo_hinh_tot_nhat = False #@param {\"type\":\"boolean\"}\n",
        "#@markdown **S·ªë k·ª∑ nguy√™n c·ªßa m√¥ h√¨nh**\n",
        "ky_nguyen = 300 #@param {type:\"slider\", min:0, max:10000, step:0.1}\n",
        "\n",
        "model_path = glob.glob(os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}_{ky_nguyen}e_*s_best_epoch.pth\")) if mo_hinh_tot_nhat else glob.glob(os.path.join(\"assets\", \"weights\", f\"{ten_mo_hinh}_{ky_nguyen}e_*s.pth\"))\n",
        "if not model_path: model_path = os.path.join(\"assets\", \"weights\", ten_mo_hinh + \".pth\")\n",
        "\n",
        "if not model_path: print(\"kh√¥ng t√¨m th·∫•y m√¥ h√¨nh!\")\n",
        "else:\n",
        "    if not os.path.exists(model_path): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh!\")\n",
        "    else:\n",
        "        try:\n",
        "            index_path = os.path.join(\"assets\", \"logs\", ten_mo_hinh,  [f for f in os.listdir(os.path.join(\"assets\", \"logs\", ten_mo_hinh)) if f.endswith('.index') and not f.startswith(\"trained\")][0])\n",
        "        except IndexError:\n",
        "            index_path = None\n",
        "            print(\"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c\")\n",
        "\n",
        "    zip_file_path = os.path.join(\"assets\", ten_mo_hinh + \".zip\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
        "        zipf.write(model_path, os.path.basename(model_path))\n",
        "        if index_path: zipf.write(index_path, os.path.basename(index_path))\n",
        "\n",
        "clear_output()\n",
        "Button(description=\"\\u2714 Xong!\", button_style=\"success\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tkqks7bO2Cye",
        "XP4ifZaG_yd5",
        "ers351v_CMGN"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
